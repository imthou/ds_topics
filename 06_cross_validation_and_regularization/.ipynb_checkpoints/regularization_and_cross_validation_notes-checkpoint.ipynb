{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization Regression And Cross Validation\n",
    "\n",
    "1) Subset Selection of Predictors\n",
    "* Ways to reduce complexity of the model:\n",
    "* $Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p + \\epsilon$\n",
    "    1. **Subset Selection** - choose subset of $p$ predictors\n",
    "    2. **Regularization** - keep $p$ predictors, shrink coefficient estimates towards 0 (some variable selection for lasso)\n",
    "    3. **Dimensionality Reduction** - project $p$ predictors into $M$-dimensional space where $M<p$\n",
    "* Subset selection techniques:\n",
    "    1. **Best subset** - try every model with every possible combination of $p$ predictors\n",
    "        * (-) computationally intensive (huge search space), especially for large amount of $p$\n",
    "        * (+) higher chance of finding models that look good on training data (low bias)\n",
    "        * (-) have little predictive power on future data (high variance) \n",
    "    2. **Stepwise selection** - in each step, a variable is considered for addition to or subtraction from $p$ to model $M$ with some criterion\n",
    "        * e.g. $M_0 \\rightarrow M_1 \\rightarrow M_2 \\rightarrow \\dots \\rightarrow M_p$\n",
    "            * where $M_1$ adds one predictor with smallest $RSS$ or largest $R^2$\n",
    "            * where $M_2$ adds one predictor with smallest $RSS$ or largest $R^2$\n",
    "        * now that there are $p$ candidate models, is using $RSS$ and $R^2$ metrics good way to determine best $p$ candidate?\n",
    "* Best ways to determine best model between multiple models:\n",
    "    1. **Cross-validation** - assesses how well a model will generalize without losing significant modelling or testing capabilites (not 70%/30% split technique). It combines averages measure of fit (prediction error) to derive a more accurate estimate of model prediction performance (k-fold cross-validation).\n",
    "    2. **Mallow's $C_p$** - takes into consideration residual sum of squares, number of predictors, and estimate of variance associated with each response in linear model\n",
    "        * equation: $C_p=\\frac{1}{n}(RSS+2\\underline{p}\\hat{\\sigma}^2)$ (minimize)\n",
    "        * $p$ = total # of parameters\n",
    "        * $\\hat{\\sigma}^2$ = an estimate of variance of error $\\epsilon$\n",
    "        * small value of $C_p$ means that model is relatively precise\n",
    "        * (-) $C_p$ approximation is only valid for large sample size\n",
    "        * (-) $C_p$ cannot handle complex collections of models as in the variable selection or feature selection problem\n",
    "    3. **Akaike Information Criterion (AIC)** - an estimate of the relative information lost when a given model is used to represent the process that generated the data. It deals with trade-off between goodness of fit and simplicity of model\n",
    "        * equation: $AIC = -2logL+2\\underline{p}$ (minimize)\n",
    "        * $L$ = maximized value of the likelihood function for model estimated\n",
    "        * the preferred model is the one with the minimum AIC value\n",
    "        * penalizes large number of estimated parameters (discourages overfitting)\n",
    "        * can show that AIC and Mallow's $C_p$ are equivalent for linear case\n",
    "    4. **Bayesian Information Criterion (BIC)** - similar to AIC, it considers like likelihood and # of parameters. The difference is that BIC penalization is larger than AIC.\n",
    "        * equation: $BIC = \\frac{1}{n}(RSS+log(n)\\underline{p}\\hat{\\sigma}^2)$ (minimize)\n",
    "        * similar to AIC, except $2$ is replaced by $log(n)$\n",
    "        * Generally, BIC exacts heavier penalty for more variables\n",
    "            * $log(n) > 2$ for $n>7$\n",
    "        * BIC assumes that data distribution is an exponential family\n",
    "        * (-) BIC is only valid for sample size $n$ much larger than the number of $k$ of parameters in the model\n",
    "        * (-) BIC cannot handle complex collections of models as in the variable selection or feature selection problem in high-dimension\n",
    "    5. **Adjusted $R^2$** - attempts to take account of $R^2$ and penalizes for extra explanatory variables added to the model\n",
    "        * equation: Adjusted $R^2= 1-\\frac{\\frac{RSS}{n-p-1}}{\\frac{TSS}{n-1}}$\n",
    "        * similar to $R^2$, but penalizes more for more variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Cross Validation\n",
    "* **Normal Cross-Validation** - randomly divide data into training set and validation set (e.g. 50/50, 60/40, 70/30, 80/20, etc.)\n",
    "    * Cross-Validation steps:\n",
    "        1. fit model on training set\n",
    "        2. use fitted model to predict responses for validation set\n",
    "        3. compute validation-set error\n",
    "            * quantitative response: typically MSE/RMSE\n",
    "            * qualitative: typically misclassification rate (e.g. accuracy, precision, recall, f1)\n",
    "    * example: fitting MPG, $Y$, from horsepower, $X$ using different polynomial fits\n",
    "    * (-) validation error can be highly variable depending on random split\n",
    "* **K-Fold Cross-Validation** - randomly divide data into $k$ folds and treat each $k$ as the validation set to measure predicted responses\n",
    "<img src=kfold_validation.png text=\"K-Fold Cross-Validation\" width=80% />\n",
    "    * K-Fold Cross-Validation steps: (run these steps $k$ times)\n",
    "        1. fit model on training set using $k-1$ folds\n",
    "        2. use fitted model to predict responses for validation set\n",
    "        3. compute validation-set error\n",
    "            * quantitative response: typically MSE/RMSE\n",
    "            * qualitative: typically misclassification rate (e.g. accuracy, precision, recall, f1)\n",
    "    * equation: $CV_{k}=\\frac{1}{k}\\sum_{i=1}^k MSE_i$\n",
    "    * example: fitting MPG, $Y$, from horsepower, $X$ using different polynomial fits\n",
    "    * (-) validation error can be highly variable depending on random split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **Bias-Variance Tradeoff** - the problem of simultaneously minimizing two sources of error, bias and variance, that prevent supervised learning algorithms from generalizing beyond their training set\n",
    "<img src=bias_variance_tradeoff.png text=\"Bias Variance Tradeoff\" width=80% />\n",
    "* situation: fit a model, $\\hat{f}(x)$, to some training data and let $(x_0,y_0)$ be a test observation from the population. If the true model is $Y=f(X)+\\epsilon$ where $f(x)=E(Y|X=x)$\n",
    "* equation: $E(y_0-\\hat{f}(x_0))^2=Var(\\hat{f}(x_0))+[Bias(\\hat{f}(x_0))]^2+Var(\\epsilon)$\n",
    "    * applies to modeling in general (not just linear regression)\n",
    "    * minimize the expected test MSE/RMSE by **reducing Variance**, $Var(\\hat{f}(x_0))$, **reducing Bias**, $Bias(\\hat{f}(x_0))$, but can't do much about Irreducible Error, $Var(\\epsilon)$\n",
    "    * generally speaking, the *more flexible* the model, the *greater the variance*\n",
    "    * $Bias(\\hat{f}(x_0)) = E[\\hat{f}(x_0)]-f(x_0)$ - difference between expected prediction of our model and correct value we are trying to predict\n",
    "    * $Var(\\hat{f}(x_0))$ - amount by which $\\hat{f}$ would change if estimated it using a different training dataset\n",
    "    * $Var(\\epsilon)$ - simply because $Y=f(X)+\\epsilon$ has irreducible error\n",
    "* reducing the complexity of the model so it stops overfitting and thus reducing the variance (how well it generalizes to different training data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) **Regularization** - solves overfitting problem in statistical models. This method reduces the values of the coefficients (aka **shrinkage**)\n",
    "* in linear regression, we find the estimates for all coefficients that minimize RSS\n",
    "    * Linear Regression: $RSS = \\sum_{i=1}^n\\Big(y_i-\\beta_0-\\sum_{j=1}^p \\beta_j x_{ij}\\Big)^2$\n",
    "* **Ridge Regression** - great technique for analyzing multiple regression data that suffers from multicollinearity\n",
    "    * minimize: $RSS + \\lambda\\sum_{i=1}^p \\beta_j^2$\n",
    "    * $\\lambda$ is tuning parameter to be determined\n",
    "    * uses L2 penalty (also known as least squares)\n",
    "    * $j$ is not zero\n",
    "* **Lasso Regression** - lasso (least absolute shrinkage and selection operator) is a regression analysis method that performs both variable selection and regularization\n",
    "    * minimize: $RSS + \\lambda\\sum_{i=1}^p \\big|\\beta_j\\big|$\n",
    "    * uses an L1 penalty (also known as least absolute deviations)\n",
    "* Ridge vs Lasso:\n",
    "<img src=ridge_vs_lasso.png text=\"Ridge vs Lasso\" width=90% />\n",
    "    * when $\\lambda=0$, we simply have linear models\n",
    "    * as $\\lambda$ increases, both models become less flexible, reducing variance, but increasing bias\n",
    "    * lasso has the advantage of variable selection as well (especially nice when $p$ is large)\n",
    "    * neither universally dominate, but in general one might expect Lasso to do better when response is function of relatively *few* predictors (however, it's not always true, so make sure to cross-validate)\n",
    "    * when two predictors are highly correlated L1 (Lasso) penalties tend to pick one of the two while L2 (Ridge) will take both and shrink the coefficients\n",
    "    * in general L1 (Lasso) penalties are better at recovering sparse signals\n",
    "    * L2 (Ridge) penalties are better at minimizing prediction error\n",
    "* Choosing $\\lambda$:\n",
    "<img src=choosing_lambda.png text=\"Choosing Lambda\" width=90% />\n",
    "    * increment $\\lambda$ per model, and then choose the *$\\lambda$ which minimizes cross-validated error*\n",
    "    * in least squares linear regression. the $\\beta$ coefficient estimates are *scale equivariant* (scale remains the same)\n",
    "        * in other words, multiplying $X_j$ by constact $c$ leads to scaling of least squares coefficient estimates by $\\frac{1}{c}$, so that $X_j\\hat{\\beta}_j$ remains the same\n",
    "    * in ridge regression, the $\\beta$ coefficient estimates can change *substantially* due to the penalty part of the ridge cost function\n",
    "        * make sure to **standardize** the predictors using:\n",
    "            * $\\tilde{x}_{ij}=\\frac{x_{ij}}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)^2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series\n",
    "\n",
    "* Objectives:\n",
    "    * Understanding key time series concepts\n",
    "    * Knowing graphical tools to analyze time series data\n",
    "    * Estimating ARIMA models using Box-Jenkins workflow\n",
    "    * Using Python's StatsModels to train and evalulate ARIMA models\n",
    "    * Describe Exponential Smoothing (ETS) model\n",
    "* (-) We are focus on forecasting the mean and not the quantiles\n",
    "* References: (arranged by increasing difficulty)\n",
    "    * Hyndman & Athanasopoulos: Forecasting: principles and practice (https://www.otexts.org/fpp)\n",
    "    * Enders: Applied Econometric Time Series\n",
    "    * Hamilton: Time Series Analysis\n",
    "    * Elliott & Timmermann: Economic forecasting\n",
    "* Python vs R:\n",
    "    * Python\n",
    "        * `pandas` - manipulate data and dates\n",
    "        * `statsmodels` - estimate core time series models\n",
    "    * R\n",
    "        * `lubridate` - manipulate dates\n",
    "        * Hyndman's `forecast` package\n",
    "        * Only serious option for ETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Time Series Concepts\n",
    "* **Time Series Data** - a sequence of observations of some quantity of interest which are collected over time\n",
    "    * e.g. GDP, price of toilet paper or a stock, demand for a good, unemployment rate, and web traffic (e.g. clicks, logins, posts, etc.)\n",
    "* Definition of Time Series - assume a timeseries, $\\{y_t\\}$, has the following properties:\n",
    "    * $y_t$ - an observation of the level of $y$ at time $t$\n",
    "    * $\\{y_t\\}$ - a collection of time series observations\n",
    "        * may extend back to $t=0$ or $t=-\\infty$, depending on the problem (e.g. $t \\in \\{0,\\dots,T\\}$)\n",
    "* Time Series Assumptions\n",
    "    * Discrete time - sampling at regular intervals (even if process is continuous)\n",
    "    * Evenly spaced observations\n",
    "    * No missing observations\n",
    "* Difficulty with Time Series - time series are hard to model because we only observe **one realization of the path** of process\n",
    "    * Often have limited data\n",
    "    * Must impose structure (e.g. assumptions about correlation) in order to model\n",
    "    * Must project beyond support of the data\n",
    "* **Components of a Time Series** - consists of several different components that can be **additive** or **multiplicative** (Time Series Decomposition)\n",
    "![timeseries_comp](timeseries_comp.png)\n",
    "    * **Trend** - the long term movement in a time series, reflects the underlying level of the series\n",
    "        * represents the underlying level of the series\n",
    "        * is relatively stable, changing gradually\n",
    "        * can be affected by business cycle\n",
    "        * may be problematic to forecast close to the turning points.\n",
    "    * **Seasonal/Periodic** - A **seasonal** pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week). Seasonality is always of a fixed and known period. Hence, seasonal time series are sometimes called **periodic** time series.\n",
    "    * **Irregular** - the residual variation remaining after the trend-cycle and seasonality have been extracted from original time series. This component appears as short-term, unsystematic fluctuations around the trend that do not follow any systematic or repeated pattern, which could be captured by seasonal component.\n",
    "        * Irregularity is formed from all unpredictable effects that affect time series (e.g. unseasonable weather, sampling errors, other errors, etc.)\n",
    "        * By and large, irregularities are considered as random variables (white noise). It is assumed that the expected value of these factors is 0 (for an additive model) or 1 (for a multiplicative model).\n",
    "    * **Cyclic** - A cyclic pattern exists when data exhibit rises and falls that are not of fixed period. The duration of these fluctuations is usually of at least 2 years. Think of business cycles which usually last several years, but where the length of the current cycle is unknown beforehand.\n",
    "* Time Series Examples:\n",
    "![timeseries_example](timeseries_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Time Series Models\n",
    "* Popular Time Series Models:\n",
    "    1. **Auto Regressive Integrated Moving Average (ARIMA(p,d,q))** -  generalization of an autoregressive moving average (ARMA) model\n",
    "        * A benchmark model\n",
    "        * Captures key aspects of time series data\n",
    "    2. **Exponential Smoothing (ETS)** - technique for smoothing time series data using the exponential window function\n",
    "        * Also known as \"State space model\"\n",
    "        * Smooths out irregular shocks to model trend and seasonality\n",
    "        * Updates forecast with linear combination of past forecast and current value\n",
    "* Mathematical Notation For Time Series:\n",
    "    * $y_t$ - the level of some value of interest at time $t$\n",
    "    * $\\epsilon_t$ - the value of a shock, $\\epsilon$, at time $t$\n",
    "    * $\\hat{y}_{t+h|t}$ - the forecast for $y_{t+h}$ based on the information available at time $t$\n",
    "* **Lags** - often models use past values to predict future\n",
    "    * ARMA Models:\n",
    "        * **Auto Regressive (AR)**: $$AR(p) \\rightarrow y_t=c + \\sum_{i=1}^p \\varphi_i y_{t-i}+\\epsilon_t$$\n",
    "            * $\\varphi_1,\\dots,\\varphi_p$ - parameters of the model\n",
    "            * $c$ - constant\n",
    "            * $\\epsilon_t$ - white noise error\n",
    "        * **Moving Average (MA)**: $$MA(q) \\rightarrow y_t=\\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\cdots + \\theta_q \\epsilon_{t-q}$$\n",
    "            * $\\theta_1,\\dots,\\theta_q$ - parameters of the model\n",
    "            * $\\epsilon_t,\\epsilon_{t-1},\\dots,\\epsilon_{t-q}$ - white noise error \n",
    "    * **Lag (Backshift) Operators** - operates on an element of a time series to produce the previous element ($L: x_t \\mapsto x_{t-1}$)\n",
    "        * **Auto Regressive (AR)**: $$AR(p) \\rightarrow y_t=c + \\sum_{i=1}^p \\varphi_i L^i y_{t}+\\epsilon_t$$\n",
    "        * **Moving Average (MA)**: $$MA(q) \\rightarrow y_t=\\mu + (1 + \\theta_1 L + \\cdots + \\theta_q L^q) \\epsilon_t$$\n",
    "* Time series concepts:\n",
    "    * Basic Statistics:\n",
    "        * **Expectation**: $$E[g(x)] = \\int g(x)f(x)dx$$ $$g(x)\\text{ - arbitrary function}$$ $$f(x)\\text{ - the probability density function}$$\n",
    "        * **Mean**: $$\\mu(x) = E[x]$$\n",
    "        * **Variance**: $$\\text{var}(x_t)=E[(x_t-\\mu(x_t))(x_t-\\mu(x_t))^T]$$ $$\\sigma^2(x_t)=\\text{var}(x_t)$$\n",
    "        * **Standard Deviation**: $$\\sigma(x_t)=\\sqrt{\\text{var}(x_t)}$$\n",
    "    * Measure of persistence of time series:\n",
    "        * **Autocovariance** - how much a lag predicts a future value of a time series $$\\text{acov}(x_t,x_{t-h})=E[(x_t-\\mu(x_t))(x_{t-h}-\\mu(x_{t-h}))^T]$$ \n",
    "            * also written as: $\\gamma(s,t)$ or $\\gamma(h)$ where $h = s - t$\n",
    "        * **Autocorrelation** - a dimensionless measure of the influence of one lag upon another. Helps determine which ARIMA model to use $$\\text{acorr}(x_t)=\\frac{\\text{acov}(x_t,x_{t+h})}{\\sigma(x_t)\\sigma(x_{t+h})}$$\n",
    "            * also written as: $\\rho(t) = \\frac{\\gamma(t)}{\\gamma(0)}$\n",
    "    * Special forms of time series (which are easier to forecast)\n",
    "        * In order to forecast, we need mean, variance, and correlation to be stable over time\n",
    "        * **Strictly Stationary** - $\\{x_t\\}$ is strictly stationary if $f(x_1,\\dots,x_t)=f(x_{1+h},\\dots,x_{t+h})\\text{ }\\forall\\text{ } h$\n",
    "        * **Weakly Stationary**\n",
    "            * mean is constant for all periods: $\\mu(x_t)=\\mu(x_{t+h})\\text{ }\\forall\\text{ } h$\n",
    "            * autocorrelation, $\\rho(s,t)$, depends on $|s-t|$\n",
    "        * **White Noise**\n",
    "            * $\\text{acov}(x_t,x_{t+h})=\\text{var}(x_t)$ iff $h=0$ and $0$ otherwise\n",
    "            * is weakly stationary\n",
    "            * is a key building block of time series models\n",
    "    * **Analog Principle** - replace expectations with sample averages when calculating statistics\n",
    "        * Intuition - Weak Law of Large Numbers\n",
    "            * mean: $E[x] = \\frac{1}{N}\\sum_{i=1}^N x_i$\n",
    "            * in general: $E[g(x)]= \\frac{1}{N}\\sum_{i=1}^N g(x_i)$\n",
    "        * Sometimes replace $N$ with $N-1$ (e.g. for variance)\n",
    "            * statistic is consistent\n",
    "            * makes estimator unbias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Auto-Regressive Integrated Moving Average (ARIMA) Model\n",
    "* $ARIMA(p,d,q)$ consists of $AR(p)$, $I(d)$, and $MA(q)$:\n",
    "    * $AR(p)$ (Auto-regressive) - model captures the persistence of **past history**\n",
    "        * means **auto-regressive of order $p$**: $AR(p) \\rightarrow y_t=c + \\sum_{i=1}^p \\varphi_i y_{t-i}+\\epsilon_t$\n",
    "        * with lag operator and polynomials: $AR(p) \\rightarrow y_t=c + \\sum_{i=1}^p \\varphi_i L^i y_{t}+\\epsilon_t$\n",
    "    * $I(d)$ (Integrated) - model captures the **non-stationary trend**\n",
    "        * means **integrated of order $d$**: $y_t=y_{t-1}+\\mu+\\epsilon_t$\n",
    "        * $d$ - how many times you must difference the series so that it is stationary\n",
    "        * usually $d\\in \\{0,1,2\\}$\n",
    "        * differencing should remove the trend component\n",
    "        * Example: random walk (with drift)\n",
    "        * Compute differences with `np.diff(n=d)` or `pd.Series.diff(periods=d)` to turn ARIMA into ARMA\n",
    "    * $MA(q)$ (Moving Average) - model captures the persistence of **past shocks**\n",
    "        * means **moving average of order $q$**: $MA(q) \\rightarrow y_t=\\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\cdots + \\theta_q \\epsilon_{t-q}$\n",
    "        * with lag operator and polynomials: $MA(q) \\rightarrow y_t=\\mu + (1 + \\theta_1 L + \\cdots + \\theta_q L^q) \\epsilon_t$\n",
    "        * **do not confuse with computing the moving average of $\\{y_t\\}$**, which is often used to aggregate data\n",
    "* ARIMA intuition:\n",
    "    * AR, I, and/or MA may be missing from a general ARIMA model\n",
    "    * May also include seasonal components: $ARIMA(p,d,q)(P,D,Q)$\n",
    "        * Can add higher order lags for seasonality\n",
    "    * If $d=0$, then ARIMA becomes ARMA\n",
    "    * If your complex algorithm isn't better than ARIMA, then use ARIMA!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Estimating ARIMA models using **Box-Jenkins**\n",
    "* Box-Jenkin algorithm:\n",
    "    1. Exploratory Data Analysis (EDA)\n",
    "        * plot time series: Autocorrelation Function (ACF), Partial Autocorrelation function (PACF)\n",
    "        * identify hypotheses, models, and data issues\n",
    "        * aggregate to an appropriate gain\n",
    "    2. Fit model(s)\n",
    "        * difference model until stationary\n",
    "    3. Examine residuals: are they white noise?\n",
    "    4. Test and evaluate on out-of-sample data\n",
    "    5. Check for:\n",
    "        * Structural breaks\n",
    "        * Seasonality and periodicity\n",
    "        * Forecast reliability for large $h$ with limited data\n",
    "* Modeling flow chart:\n",
    "![modeling_flow](modeling_flow.png)\n",
    "* Plot data to develop understanding of data and possible models:\n",
    "    * Key diagnostic plots:\n",
    "        * Plot time series: $y_t$ vs. $t$\n",
    "        * Plot autocorrelation function (ACF): $\\rho(h)$ vs. $h$\n",
    "        * Plot partial autocorrelation function (PACF)\n",
    "    * Repeat for first and second differences, if necessary:\n",
    "        * Compute differences with `np.diff(n=d)` or `pd.Series.diff(periods=d)`\n",
    "        * Transform series, if necessary e.g. $y_t \\rightarrow log(y_t)$\n",
    "        * Check stationarity e.g. no trend and constant variance\n",
    "* **Autocorrelation Function (ACF)** - shows likely order of the $MA(q)$ part of the $ARIMA(p,d,q)$ model:\n",
    "    * Plots $\\rho(h)$ vs. lags $h$\n",
    "    * Find largest significant spike\n",
    "    * Consider order $q$, where $q=\\text{largest lag}$\n",
    "    * Sample ACF code:\n",
    "    ```python\n",
    "    import statsmodels.api as sm\n",
    "    data = sm.tsa.arma_generate_sample(ar=[0.7,0.0,0.3], ma=[0.2,-0.1], nsample=100)\n",
    "    sm.graphics.tsa.plot_acf(data, lags=28, alpha=0.05)\n",
    "    plt.show()\n",
    "    ```\n",
    "* **Partial Autocorrelation Function (PACF)** - shows likely order of the $AR(p)$ part of the $ARIMA(p,d,q)$ model:\n",
    "    * Plots partial autocorrelation vs. lags $h$\n",
    "    * Partial autocorrelation uses a regression method to compute effect of just a single lag but not intermediate lags like ACF\n",
    "    * Consider order $p$, where $p=\\text{largest lag}$\n",
    "    * Sample PACF code:\n",
    "    ```python\n",
    "    import statsmodels.api as sm\n",
    "    data = sm.tsa.arma_generate_sample(ar=[0.7,0.0,0.3], ma=[0.2,-0.1], nsample=100)\n",
    "    sm.graphics.tsa.plot_pacf(data, lags=28, alpha=0.05)\n",
    "    plt.show()\n",
    "    ```\n",
    "* Helper Function For Plotting ACF and PACF\n",
    "    * Helper function\n",
    "    ```python\n",
    "    def ts_diag_plot(data, lags=28):\n",
    "        fig = plt.figure(figsize=(15,10))\n",
    "        ax1 = fig.add_subplot(311)\n",
    "        ax1.plot(data)\n",
    "        ax1.set_title('y_t vs. t')\n",
    "        ax2 = fig.add_subplot(312)\n",
    "        sm.graphics.tsa.plot_acf(data, lags=lags, ax=ax2) \n",
    "        ax3 = fig.add_subplot(313) \n",
    "        sm.graphics.tsa.plot_pacf(data, lags=lags, ax=ax3) \n",
    "        fig.show()\n",
    "        return fig\n",
    "\n",
    "    from tsplot import ts_diag_plot\n",
    "    fake = sm.tsa.arma_generate_sample(ar=[ 0.7, 0.0, 0.3], ma=[0.2, -0.1], nsample=100)\n",
    "    fig = ts_diag_plot(fake)\n",
    "    ```\n",
    "    * Plot of ACF and PACF\n",
    "    ![acf_pacf](acf_pacf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Question To Think About For Time Series Data\n",
    "* Is it stationary?\n",
    "* Is there a trend?\n",
    "* Is the variance stable?\n",
    "* Are there seasonal or periodic components?\n",
    "* What $AR$ and $MA$ terms are likely present?\n",
    "* Are there structural breaks in the data?\n",
    "* Do I have enough data to forecast at horizon $h$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Stabilizing the Time Series\n",
    "* If you need to stabilize the time series before estimating a model:\n",
    "    * Transform data to **stabilize variance**:\n",
    "        * $y_t \\rightarrow log(y_t)$\n",
    "        * Verify via **Box-Cox test**\n",
    "        * Verify by plotting\n",
    "    * Transform data so **series is stationary**:\n",
    "        * Compute first or second difference\n",
    "        * $y_t \\rightarrow \\Delta y_t$ or $y_t \\rightarrow \\Delta^2 y_t$\n",
    "        * Verify by **Portmanteau test**    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Fitting an ARIMA model\n",
    "* Steps to fitting a model:\n",
    "    1. Split data into train set (earlier observations) and test set (later observations)\n",
    "    2. To forecast at horizon, $h$, should have at least $3h$ observations to train plus $h$ observations to test:\n",
    "        * e.g. you **cannot** forecast demand in two years if you only have three months of data\n",
    "        * If these conditions are violated, you need a \"panel of experts\"\n",
    "        * More data is better, especially if seasonality is present\n",
    "    3. To identify optimal order of model:\n",
    "        * Examine ACF and PACF\n",
    "        * Difference until stationary\n",
    "        * Number of differences is order $d$ for $l(d)$\n",
    "        * Use `sm.tsa.arma_order_select_ic` to generate compare several models\n",
    "        * Use cross validation\n",
    "* Code For Fitting ARIMA model:\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "data = sm.datasets.macrodata.load_pandas()\n",
    "df = data.data\n",
    "df.index = pd.Index(sm.tsa.datetools.dates_from_range('1959Q1', '2009Q3'))\n",
    "y = df.m1\n",
    "X = df[['realgdp', 'cpi']]\n",
    "model = sm.tsa.ARIMA(endog=y, order=[1,1,1])\n",
    "# model2 = sm.tsa.ARIMA(endog=y, order=[1,1,1], exog=X) \n",
    "results = model.fit()\n",
    "results.summary()\n",
    "```\n",
    "* Results of Fitted ARIMA model:\n",
    "![arima_results](arima_results.png)\n",
    "* Advanced ARIMA techniques (for more complicated situations):\n",
    "    * Add **Fourier terms** to capture **periodic** behavior\n",
    "    * Add other **covariates** which can improve prediction\n",
    "    * Use **Vector Autoregressivev Integrated Moving Average (VARIMA)** model to capture dynamics of a system of equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Forecasting with Prediction Intervals\n",
    "* **Prediction Intervals** - contains future realization of the mean $y_{t+h}$ with probability $1-\\alpha$ and increases the further you forecast into the future\n",
    "    * A forecast of $\\{y_t\\}$ at time $t+h$ computes:\n",
    "        * $\\hat{y}_{t+h|t}$ - the expected mean of $y_t$ at time $t+h$ conditional on the information available at $t$\n",
    "    * A prediction interval is **not a confidence interval**:\n",
    "        * A **prediction interval** contains the future realization of a random variable with percent certainty: $Pr=1-\\alpha$\n",
    "        * A **confidence interval** contains the true value of a parameter with percent certainty: $Pr=1-\\alpha$\n",
    "* **Forecasting** - can use `results.forecast` to compute out of sample predictions:\n",
    "    * Use `alpha`, $\\alpha$, to choose appropriate prediction interval, e.g. 80%, 90%, 95%, etc.\n",
    "    * Do not use the prediction interval to forecast quantiles of $\\hat{y}_{t+h|t}$\n",
    "    * Can supply (forecasted) value of exogenous predictors\n",
    "    ```python\n",
    "    y_hat, stderr, pred_int = results.forecast(steps=h, alpha=0.05)\n",
    "    ```\n",
    "    * Prediction plot includes a **prediction interval**:\n",
    "        * Contains future realization of $y_{t+h}$ with probability $1-\\alpha$\n",
    "        * A prediction interval is **not** a confidence interval\n",
    "        ```python\n",
    "        results.plot_predict('2009Q3', '2014Q4', dynamic=True, plot_insample=True)\n",
    "        plt.show()\n",
    "        ```\n",
    "    ![prediction_intervals](prediction_intervals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Evaluating Forecasting Model\n",
    "* Evaluating Steps:\n",
    "    * Check residuals are white noise:\n",
    "        * Examine ACF and PACF\n",
    "        * Compute **Portmanteau test (Box-Pierce, Box-Ljung)** to see if residuals are correlated\n",
    "    * Check solver converged!\n",
    "    * Remember that simple models often outperform fancy models on new data\n",
    "    * Compare any forecast against the benchmark forecast:\n",
    "        * Choose a benchmark such as mean or random walk with drift\n",
    "        * Fit model on training set and evaluate on test set\n",
    "        * To compare multiple forecasts, use a sliding window\n",
    "* Common Evaluation Metrics:\n",
    "    * **Root Mean Squared Error (RMSE)**: $$RMSE=\\sqrt{\\frac{1}{H}\\sum_{i=1}^h (y_{t+h}-\\hat{y}_{t+h|t})^2}$$\n",
    "    * **Mean Absolute Error (MAE)**: $$MAE=\\frac{1}{H}\\sum_{i=1}^h |y_{t+h}-\\hat{y}_{t+h|t}|$$\n",
    "    * **Mean Absolute Percentage Error (MAPE)**: $$MAPE=\\frac{1}{H}\\sum_{i=1}^h \\Big|\\frac{y_{t+h}-\\hat{y}_{t+h|t}}{y_{t+h}}\\Big|$$\n",
    "* Model Selection - use **information criterion** to evaluate models\n",
    "    * Several information criteria exists: **AIC**, **AICc**, **BIC**\n",
    "        * Essentially, log-likelihood plus penality for adding parameters\n",
    "        * Measures fit vs. parsimony of model\n",
    "        * Different criteria have different finite sample properties\n",
    "    * Choose model with **lowest** information criterion\n",
    "    * Especially helpful if you have **limited** data\n",
    "    * Popular, pre-ML method, but consider **cross-validation** if you have enough data\n",
    "* Tips For Forecasting\n",
    "    * Work at the appropriate level of aggregation (grain):\n",
    "        * Don't use 5 minute resolution data to forecast at $h = \\text{one month}$\n",
    "    * Don't forecast beyond what the data will support\n",
    "        * Should have $4h$ amount of data to forecast at horizon $h$\n",
    "    * Err on the side of simplicity\n",
    "    * Or, take a machine learning approach:\n",
    "        * Try a set of lags and differences plus other predictors\n",
    "        * Use regularization and/or variable selection\n",
    "        * See Taieb & Hyndman for an approach which uses boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) **Exponential Smoothing (ETS)** model\n",
    "* Benefits of ETS model:\n",
    "    * Robust performance\n",
    "    * Easy to explain to non-technical stakeholders\n",
    "    * Easy to estimate with limited computational resources\n",
    "    * Forecast well because of parsimony\n",
    "* ETS model consists of smoothing equations for:\n",
    "    * Forecast\n",
    "    * Level\n",
    "    * Trend (optional)\n",
    "    * Seasonality (optional)\n",
    "* Model Remarks:\n",
    "    * Can use either an additive or multiplicative specification\n",
    "    * Can use a state space formulation\n",
    "    * Usually a three-character string identifying method using the framework terminology of Hyndman et al. (2002) and Hyndman et al. (2008):\n",
    "        * In all cases, \"N\" = none, \"A\" = additive, \"M\" = multiplicative and \"Z\" = automatically selected\n",
    "        * The first letter denotes the **error** type (\"A\", \"M\" or \"Z\") \n",
    "        * The second letter denotes the **trend** type (\"N\",\"A\",\"M\" or \"Z\")\n",
    "        * The third letter denotes the **seasonality** type (\"N\",\"A\",\"M\" or \"Z\") \n",
    "        * For example, \"ANN\" is simple exponential smoothing with additive errors, \"MAM\" is multiplicative Holt-Winters' method with multiplicative errors, and so on.\n",
    "* Hyndman's Taxonomy:\n",
    "    * Hyndman categorizes exponential smoothing models as ETS:\n",
    "        * $E$ for type of **error**\n",
    "        * $T$ for type of **trend**\n",
    "        * $S$ for type of **seasonality**\n",
    "    * Typical values are:\n",
    "        * $A$ for **additive**\n",
    "        * $M$ for **multiplicative**\n",
    "        * $N$ for **none**\n",
    "        * $A_d$ for **additive damped**\n",
    "        * $M_d$ for **multiplicative damped**\n",
    "    * Example ETS model types:\n",
    "        * ETS(AAN)\n",
    "            * Has additive error and trend but no seasonality\n",
    "            * Simple exponential smoothing\n",
    "            * e.g. **Holt's Linear method**, \"double exponential smoothing\"\n",
    "        * ETS(AAA)\n",
    "            * **Holt-Winters' method**\n",
    "            * Adds seasonality\n",
    "* Popular ETS Models:\n",
    "    * **Simple Exponential Smoothing ETS(ANN)** - updates forecast based on latest realization of $y_t$\n",
    "        * Forecast equation: $\\hat{y}_{t+1|t}=\\mathcal{l}_t$\n",
    "        * Level equation: $\\mathcal{l}_t = \\alpha y_t + (1-\\alpha) \\mathcal{l}_{t-1}$\n",
    "        * If $y_t=\\hat{y}_{t|t-1}+\\epsilon_t$, can use **error correction** formulation:\n",
    "            * $y_t=\\mathcal{l}_{t-1}+\\epsilon_t$\n",
    "            * $\\mathcal{l}_t=\\mathcal{l}_{t-1}+\\alpha \\epsilon_t$\n",
    "    * **Holt's Linear Model ETS(AAN)** - adds slope to the model to better handle a trend\n",
    "        * Forecast equation: $\\hat{t}_{t+h|t}=\\mathcal{l}_t+h b_t$\n",
    "        * Level equation: $\\mathcal{l}_t=\\alpha y_t + (1-\\alpha)(\\mathcal{l}_{t-1}+b_{t-1})$\n",
    "        * Trend equation: $b_t=\\beta^*(\\mathcal{l}_t-\\mathcal{l}_{t-1})+(1-\\beta^*)b_{t-1}$\n",
    "* Python support for ETS model: (partial)\n",
    "    * Python package: `pandas.stats.moments.ewma`\n",
    "    * Is user unfriendly\n",
    "    * Best to use R's `ets` function in the `forecast` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) ETS vs. ARIMA\n",
    "* ARIMA features and benefits:\n",
    "    * Benchmark model for almost a century\n",
    "    * Much easier to estimate with modern computational resources\n",
    "    * Easy to diagnose models graphically\n",
    "    * Easy to fit using Box-Jenkins methodology\n",
    "* ETS features and benefits:\n",
    "    * Can handle non-linear and non-stationary processes\n",
    "    * Can be computed with limited computational resources\n",
    "    * Not always a subset of ARIMA\n",
    "    * Easier to explain to non-technical stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Testing Understanding of Time Series\n",
    "* What are the steps in the Box-Jenkins' approach?\n",
    "* How much data do I need to forecast at horizon $h$?\n",
    "* How should I evaluate a forecast?\n",
    "* What are the benefits of ARIMA vs ETS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

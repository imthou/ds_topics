{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power & Bayesian Inference\n",
    "\n",
    "1) Power (Frequentist Hypothesis Testing)\n",
    "* What is Power?\n",
    "    * **power** of a hypothesis test is the probability that the test *correctly rejects the null hypothesis $(H_0)$*\n",
    "        * which test do we like better?\n",
    "            * test 1: $\\alpha=0.05,$ power = $0.8$\n",
    "            * test 2: $\\alpha=0.05,$ power = $0.3$\n",
    "    *  $\\begin{align} \\text{Power}\n",
    "       & = P(\\text{Reject } H_0 \\big| H_0 \\text{ is false}) \\\\\n",
    "       & = P(\\text{Reject } H_0 \\big| H_A \\text{ is true}) \\\\\n",
    "       & = 1 - \\text{Type II error} \\\\\n",
    "       & = 1 - \\beta \n",
    "    \\end{align}$\n",
    "    * what's the chance of rejecting the null when the null hypothesis is false?\n",
    "        * in other words, what's the probability of detecting a real effect?\n",
    "    * Illustrating power based on hypothesis test\n",
    "![power_in_hypo](http://pip.ucalgary.ca/psyc-312/repeated-measures/within-subjects-designs/images/Power.png)\n",
    "        1. Draw two curves based on $H_0$ (get $\\mu_0$) and sample data ($\\bar{x}$ and $s$)\n",
    "        2. Define rejection region (color region until you hit $\\alpha$)\n",
    "        3. Calculate Type II error region, $\\beta$, by computing area to the left of decision boundary\n",
    "        4. Power is $1-\\beta$\n",
    "    * Factors influencing Power - what happens to power as we **increase** each of the following factors?\n",
    "        1. $\\uparrow \\alpha$ \n",
    "        2. $\\uparrow$ effect size (distance between two hypothesis means, $H_0$ and $H_1$)\n",
    "        3. $\\uparrow$ standard deviation, $s$\n",
    "        4. $\\uparrow$ sample size, $n$\n",
    "    * Influence of Power analysis on Experimental Design\n",
    "        * usually need to decide what *sample size* to collect\n",
    "        * especially true if important to minimize number of samples due to:\n",
    "            * painful or risky (e.g. new surgical procedure)\n",
    "            * costly or expensive (e.g. paying test subjects)\n",
    "        * *power analysis can allow you to determine the sample size needed to detect a particular effect*\n",
    "* Calculating Power\n",
    "    * example: One-sample Test of Mean\n",
    "        * $H_0 : \\mu = \\mu_0$\n",
    "        * $H_1 : \\mu = \\mu_1(> \\mu_0)$\n",
    "        * $\\text{Power} = P(\\text{Reject } H_0 \\big| H_0 \\text{ is false})$\n",
    "    * find the critical value, under $H_0$, beyond which we would reject $H_0$:\n",
    "        * $X^* = \\mu_0 + Z^* \\frac{s}{\\sqrt{n}}$\n",
    "    * then, find the corresponding probability under $H_1$\n",
    "        * $\\text{Power} = P(X > X^*\\big|H_1) = P(Z > \\frac{X^*-\\mu_1}{\\frac{s}{\\sqrt{n}}})$\n",
    "    * power calculation steps:\n",
    "![power_in_hypo_2](https://i.stack.imgur.com/R0ncP.png)\n",
    "        1. decide the critical value for the test statistic (in general)\n",
    "            * $Z^* = \\pm 1.96$ for two-sided test\n",
    "            * $Z^* = +1.64 \\text{ or} -1.64$ for one-sided test\n",
    "        2. calculate the corresponding value under the null distribution ($X^*$)\n",
    "        3. find the tail probability of the above value under the alternative distribution (power)\n",
    "* Calculating the sample size, $n$, for a given level of Power\n",
    "    * what happens when we increase the sample size?\n",
    "    * what if we do not know the true mean and want to collect a larger sample for the test?\n",
    "    * requirements:\n",
    "        * a mean associated with the null hypothesis $\\mu_0$\n",
    "        * estimate of the population mean $\\mu_1$\n",
    "        * estimate of the population standard deviation $\\sigma$\n",
    "        * a fixed significance level ($\\alpha$, often 5%)\n",
    "        * a desired power level ($1-\\beta$, often 80%)\n",
    "    * derive the value for $n$ from the power calculation:\n",
    "        * $n = ((Z_{1-\\text{power}}+Z^*)\\frac{s}{\\mu_1-\\mu_0})^2$\n",
    "        * both $Z$ should have the same sign\n",
    "    * sample size calculation steps:\n",
    "        1. obtain some sort of initial estimation of the parameter/effect we are trying to test (e.g. a pilot experiment)\n",
    "        2. decide on the desired power of the test (e.g. power = 0.8)\n",
    "        3. calculate the sample size using the initial estimation and desired power\n",
    "* Relationship to A/B Testing\n",
    "    * A/B Testing is essentially Two-Sample Hypothesis testing\n",
    "    * in practice, we often conduct a small pilot experiment to estimate the sample size for a given power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Bayesian Inference\n",
    "* Frequentist vs. Bayesian\n",
    "    * Probability differences:\n",
    "        * **Frequentist Probability** - \"Long Run\" frequency of an outcome\n",
    "        * **Subjective Probability** - A measure of degree of belief\n",
    "        * **Bayesian Probability** - combination of Frequentist and Subjective probability\n",
    "    * Interpretation of experimental results:\n",
    "        * Experiment 1: a fine classical musician says he is able to distinguish Haydn from Mozart. Small excerpts are selected at random and played for musician. Musician makes 10 correct guesses in exactly 10 trials\n",
    "            * Frequentist: Very skilled musician; have confidence about musician's ability to distinguish Haydn from Mozart\n",
    "            * Bayesian: Not certain, I have some prior confidence of musician's ability\n",
    "        * Experiment 2: Drunk man he can correctly guess face of coin will fall down mid air. Coins are tossed and drunken man shouts out guesses while coins are mid air. Drunk man correctly guesses outcomes of the 10 throws\n",
    "            * Frequentist: Very skilled drunk man; have confidence about drunk's ability to predict coin tosses\n",
    "            * Bayesian: Not certain, I have some prior confidence about the drunk's ability\n",
    "        * Experiment 3: Detector that rolls two dice to see if sun has gone nova. Will lie if rolls two sixes, otherwise tells the truth\n",
    "            * Frequentist: Probability of this result happening by chance is $\\frac{1}{36} = 0.027 < 0.05 (p)$, so conclusion is that the sun has exploded with 95% confidence\n",
    "            * Bayesian: I don't believe that happened based on my prior confidence\n",
    "* Prior, Likelihood, and Posterior Distributions\n",
    "    * Update beliefs after considering new evidence\n",
    "    * Probability as measure of believability in event\n",
    "    * **A priori** - a prior belief distribution for outcomes\n",
    "        * e.g. prior belief of musician's ability to distinguish Haydn from Mozart\n",
    "    * Posterior distribution similarity to Bayes Rule\n",
    "        * Bayes Rule: $P(A|B) = \\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|A^c)P(A^c)}$\n",
    "        * Posterior Distribution: $\\pi(\\theta|x) = \\frac{f(x|\\theta)\\pi(\\theta)}{f(x|\\bar{\\theta})\\pi(\\bar{\\theta})d(\\bar{\\theta})}$\n",
    "    * Bayesian Inference equation parts:\n",
    "        * **Prior distribution** - describe our current (prior) knowledge about $\\theta$ (or $A$). Can be subjective\n",
    "        * **Likelihood** - distribution for the data (as a function of the parameter)\n",
    "        * **Posterior distribution** - our updated knowledge about $\\theta$ (or $A$) after seeing the data \n",
    "![bayesian_update](bayesian_updating_coin_toss.png)\n",
    "* **Maximum a posteriori (MAP)** - mode of posterior distribution\n",
    "    * For MAP, we assume a prior $g$ over $\\Theta$, and go one step further to get the posterior\n",
    "    * Simply get Posterior distribution using Bayes\n",
    "    * MLE: $\\hat{\\theta}_{mle} = argmax_{\\theta \\in \\Theta}f(x|\\theta) = argmax_{\\theta \\in \\Theta}$ $logL(\\theta|x_1,\\dots,x_n)$\n",
    "    * MAP: $f(\\theta|x)=\\frac{f(x|\\theta)g(\\theta)}{\\int_{\\upsilon \\in \\Theta}f(x|\\upsilon)g(\\upsilon)d\\upsilon}$\n",
    "    * $\\hat{\\theta}_{map} = argmax_{\\theta \\in \\Theta}$ $\\frac{f(x|\\theta)g(\\theta)}{\\int_{\\upsilon \\in \\Theta}f(x|\\upsilon)g(\\upsilon)d\\upsilon} = argmax_{\\theta \\in \\Theta} f(x|\\theta)g(\\theta)$\n",
    "![map](https://www.probabilitycourse.com/images/chapter9/MAP.png)\n",
    "* Famous examples of Bayesian Inference:\n",
    "    1. Monty Hall Problem - Assume that a room is equipped with three doors. Behind two are goats, and behind the third is a shiny new car. You are asked to pick a door, and will win whatever is behind it. Let's say you pick door 1. Before the door is opened, however, someone who knows what's behind the doors (Monty Hall) opens one of the other two doors, revealing a goat, and asks you if you wish to change your selection to the third door (i.e., the door which neither you picked nor he opened). The Monty Hall problem is deciding whether you do\n",
    "        * Two choices are 50-50 when you know nothing about them\n",
    "        * Monty helps us by “filtering” the bad choices on the other side. It’s a choice of a random guess and the “Champ door” that’s the best on the other side.\n",
    "        * In general, more information means you re-evaluate your choices.\n",
    "        * Similar problem: A Bayesian Filter improves as it gets more information about whether messages are spam or not. You don’t want to stay static with your initial training set of data.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

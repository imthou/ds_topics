{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression and EDA\n",
    "\n",
    "1) **Exploratory Data Analysis (EDA)**\n",
    "* Objectives of EDA\n",
    "    1. *Suggest hypotheses* about the causes of observed phenomena\n",
    "    2. *Assess assumptions* on which inference is based\n",
    "    3. Support *selection of appropriate tools and techniques*\n",
    "    4. Provide *basis for further data* collection\n",
    "* Data Mungling/Cleaning Objectives:\n",
    "    * Making sure that the data makes sense\n",
    "    * Checking for missing data (do you need to impute data?)\n",
    "    * Looking for outliers/anomalies\n",
    "    * Making data type conversions\n",
    "    * Transforming data (e.g. aggregation)\n",
    "    * Encoding, decoding, recoding data so that categorical variables are usable in machine learning algo\n",
    "    * Renaming variables to normalize data\n",
    "    * Merging data (e.g. joining tables together)\n",
    "* Types of variables\n",
    "    * Qualitative or Categorical\n",
    "    * Quantative or Numerical\n",
    "* Number of variables\n",
    "    * Univariate (1)\n",
    "    * Bivariate (2)\n",
    "    * Multivariate (3 or more)\n",
    "* Types of plots to use for specified variable analysis\n",
    "    * Univariate, numeric: \n",
    "        * **Histogram/KDE**\n",
    "            * shows center, variability (spread), skewness\n",
    "            * any outliers\n",
    "            * choose bin value carefully\n",
    "        * **Boxplots**\n",
    "            * median, IQR, range\n",
    "            * any outliers\n",
    "            * but, doesn't show distributional shape (soln: choose violinplot)\n",
    "    * Univariate, categorical:\n",
    "        * **Bar charts**\n",
    "            * univariate\n",
    "            * univariate by type\n",
    "                * side by side bar chart\n",
    "                * stacked bar chart\n",
    "    * Bivariate, numeric vs numeric:\n",
    "        * **Scatterplots**\n",
    "            * understanding trends\n",
    "            * sometimes it could be useful to bin quantatively variables using boxplots\n",
    "    * Bivariate, numeric vs categorical:\n",
    "        * **Overlay Two Histograms**\n",
    "            * comparing distribution properties\n",
    "        * **Multiple Boxplots**\n",
    "            * compare medium, IQR, range, and outliers\n",
    "    * Bivariate, categorical vs categorical:\n",
    "        * **Heatmap**\n",
    "            * use pd.crosstab to visualize difference in percentages    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **Simple Linear Regression**\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/400px-Linear_regression.svg.png text=\"Simple Linear Regression\" width=70% />\n",
    "<img src=https://daviddalpiaz.github.io/appliedstats/images/model.jpg text=\"Simple Linear Regression with Distribution\" width=70% />\n",
    "* Model: $Y=\\beta_0 + \\beta_1 X + \\epsilon$\n",
    "    * $\\beta_0$ and $\\beta_1$ are unknown constants that represent the intercept and slope\n",
    "    * $\\epsilon$ is the error term\n",
    "        * $\\epsilon$ assumes random variables are independent and identically distributed on Normal distribution\n",
    "        * $\\epsilon$ ~ $Normal(0, \\sigma^2)$\n",
    "    * Fitted Values from Model: $\\hat{y} \\rightarrow$ $\\hat{y}=\\hat{B}_0 + \\hat{B}_1 x$\n",
    "        * $\\hat{B}_0$ and $\\hat{B}_1$ are model coefficient estimates for the world (presumed)\n",
    "        * $\\hat{y}$ indicates the prediction of $Y$ based on $X=x$\n",
    "* want to minimize error: $e_i=y_i-\\hat{y}_i=y_i-\\hat{B}_0-\\hat{B}_1x_i$\n",
    "    * **Residual Sum of Square (RSS)** - the sum of the squares of residuals (deviations predicted from actual empirical values of data). It is a measure of the discrepancy between the data and an estimation model. A small RSS indicates a tight fit of the model to the data. \n",
    "    * $\\begin{align} RSS \n",
    "        & = e_1^2 + e_2^2 + \\cdots + e_n^2 \\\\\n",
    "        & = (y_1-\\hat{B}_0-\\hat{B}_1x_1)^2+(y_2-\\hat{B}_0-\\hat{B}_1x_2)^2+\\cdots+(y_n-\\hat{B}_0-\\hat{B}_1x_n)^2 \\\\\n",
    "        & = \\sum_{i=1}^n (y_i-(\\hat{B}_0 + \\hat{B}_1x_i))^2 \\\\\n",
    "        \\end{align}$\n",
    "    * estimates $\\hat{B}_0$ and $\\hat{B}_1$ minimze RSS\n",
    "        * $\\hat{B}_0=\\bar{y}-\\hat{B}_1\\bar{x}$\n",
    "        * $\\hat{B}_1=\\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2}$\n",
    "* Coefficient estimate - determine with hypothesis test if $X$ (feature) has a effect on predictor $Y$\n",
    "    * $SE(\\hat{\\beta}_0)^2 = \\sigma^2 \\big[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\big]$\n",
    "    * $SE(\\hat{\\beta}_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i-\\bar{x})^2}$\n",
    "    * $\\sigma^2 = Var(\\epsilon)$ finding the spread for error\n",
    "\n",
    "|                     |                                         One-sample mean test                                         |                                    One-sample coefficient test                                   |\n",
    "|:-------------------:|:----------------------------------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------:|\n",
    "|   Setup Hypothesis  |                                       $H_0: \\mu = \\mu_0 = 100$                                       |                                        $H_0: \\beta_1 = 0$                                        |\n",
    "|   Sample Statistic  |                                               $\\bar{x}$                                              |                                            $\\hat{B}_1$                                           |\n",
    "| Test Statistic      | $t=\\frac{\\bar{x}-\\mu_0}{\\frac{s}{\\sqrt{n}}}$                                                         | $t=\\frac{\\hat{B}_1-0}{SE(\\hat{B}_1)}$                                                            |\n",
    "| Confidence Interval | $(\\bar{x}-t_{\\frac{\\alpha}{2}}*\\frac{s}{\\sqrt{n}}, \\bar{x}+t_{\\frac{\\alpha}{2}}*\\frac{s}{\\sqrt{n}})$ | $(\\hat{B}_1-t_{\\frac{\\alpha}{2}}*SE(\\hat{B}_1), \\hat{B}_1+t_{\\frac{\\alpha}{2}}*SE(\\hat{B}_1))$ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **Multiple Linear Regression**\n",
    "* Model: $Y = \\beta_0+\\beta_1X_1+\\beta_2X_2+\\cdots+\\beta_pX_p+\\epsilon$\n",
    "* Fitted Values From Model: $\\hat{y} = \\hat{\\beta}_0+\\hat{\\beta}_1X_1+\\hat{\\beta}_2X_2+\\cdots+\\hat{\\beta}_pX_p$\n",
    "* Residual Sum of Squares: $\\begin{align} RSS\n",
    "    & = \\sum_{i=1}^n(y_i-\\hat{y}_i)^2 \\\\\n",
    "    & = \\sum_{i=1}^n(y_i-\\hat{B}_0-\\hat{B}_1x_{i1}-\\hat{B}_2x_{i2}-\\cdots-\\hat{B}_px_{ip})^2 \\\\\n",
    "    \\end{align}$\n",
    "* Coefficient Estimates: $\\hat{B}=(X^TX)^{-1}X^Ty$\n",
    "<img src=multiple_linear_regression.png text=\"Simple Linear Regression with Distribution\" width=90% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Assessing Accuracy and Comparing Models\n",
    "* Metrics for assessing accuracy:\n",
    "    1. **Residual Sum of Squares (RSS)** - (aka Sum of the Squared Residuals) the sum of the squares of residuals (deviations predicted from actual empirical values of data)\n",
    "        * equation: $RSS=\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$\n",
    "        * example: $RSS = 1520123.11$\n",
    "            * really meaningless number\n",
    "            * this RSS value grows with $n$\n",
    "            * measured in the *units of the response variable*, $y$\n",
    "                * e.g. think $y$ in dollars vs. $y$ in millions of dollars\n",
    "    2. **Mean Squared Error (MSE)** - (aka Mean of the Squared of the Residuals) an estimator that measures the average of squares of the errors or deviations. It measures the quality of an estimator as a second moment (which incorporates variance and its bias)\n",
    "        * equation: $MSE=\\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$\n",
    "    3. **Root Mean Squared Error (RMSE)** - taking the square root of MSE is analagous as takin the sqrt of variance to obtain standard deviation. It represents the sample standard deviation of the differences between predicted and observed values\n",
    "        * equation: $RMSE=\\sqrt{\\frac{\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{n}}$\n",
    "    4. **Residual Standard Error (RSE)** - standard deviation of points formed around a linear function, and is an estimate of the accuracy of the dependent variable being measured. Can roughly think of as average amount that response will deviate from regression line\n",
    "        * equation: $RSE=\\sqrt{\\frac{1}{n-p-1}RSS}=\\sqrt{\\frac{(y_i-\\hat{y}_i)^2}{n-p-1}}$\n",
    "    5. **R-Squared (R$^2$)** (aka Coefficient of determination or Proportion of Variance Explained) - the proportion of the variance in the dependent variable that is predictable from the independent variable(s). A statistical measure of how close the data are to the fitted regression line. \n",
    "        * equation: $R^2=\\frac{TSS-RSS}{TSS}=1-\\frac{RSS}{TSS}$ where Total Sum of Squares: $TSS=\\sum_{i=1}^n (y_i-\\bar{y})^2$\n",
    "        * independent of scale of y\n",
    "        * 0% indicates that the model explains none of the variability of the response data around its mean\n",
    "    6. **F-test** - used to compare difference between different models with different number of variables to determine how important the new variables are\n",
    "        * equation: $F=\\frac{\\frac{RSS_{reduced}-RSS_{full}}{p_{full}-p_{reduced}}}{\\frac{RSS_{full}}{n-p_{full}-1}}$\n",
    "            * where $F$ has degrees of freedom $(p_{full}-p_{reduced}), (n-p_{full}-1)$\n",
    "        * example: predict $Y$ (MPG) and suspect height and color might not be very important variables\n",
    "        * assume $\\alpha=0.05$\n",
    "            1. setting up comparison models:\n",
    "                * $m_{reduced}: Y$~$\\beta_0+\\beta_{weight}+\\beta_{modelyear}+\\beta_{cartype}$\n",
    "                * $m_{full}: Y$~$\\beta_0+\\beta_{weight}+\\beta_{modelyear}+\\beta_{cartype}+\\beta_{height}+\\beta_{color}$\n",
    "            2. compute F-statistic:\n",
    "                * $F = 2.23$\n",
    "                * notice that if height and color really don't matter much: $(RSS_{reduced}-RSS_{full})$ will be small $\\rightarrow$ F-statistic will be small\n",
    "            3. compute p-value:\n",
    "                * p-value = 0.1241\n",
    "                * if $p < 0.05$ reject null (that height and color don't matter)\n",
    "                * if $p >= 0.05$ fail to reject null (that height and color don't matter)\n",
    "        * comparing models using the F-test:\n",
    "            * is my mode useful at all? e.g. is at least one of the my predictors $X_1,X_2,\\dots,X_p$ useful in predicting the response?\n",
    "                * $H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$\n",
    "                * $H_A:$ at least one of the $\\beta_j$ is non-zero\n",
    "                * $F=F_{p,n-p-1}$~$\\frac{\\frac{TSS-RSS}{p}}{\\frac{RSS}{n-p-1}}$\n",
    "            * equivalence to t-test in the regression output\n",
    "                * $m_{reduced}: Y$~$\\beta_0+\\beta_{weight}+\\beta_{modelyear}$\n",
    "                * $m_{full}: Y$~$\\beta_0+\\beta_{weight}+\\beta_{modelyear}+\\beta_{cartype}$\n",
    "* Interpretation:\n",
    "<img src=ols_regression_results.png text=\"Simple Linear Regression with Distribution\" width=90% />\n",
    "    * **R-squared** - proportion of variance explained by model is 93.3%\n",
    "    * **Prob(F-statistic)** - measure of the significance of the fit (determining how useful the model is): $6.30e-27$\n",
    "        * you want a low value to determine that the model is useful\n",
    "    * **Confidence Interval of Coefficient** - there is an approximately 95% chance that $[0.275, 0.693]$ will contain the true value of $\\beta_2$\n",
    "    * **p-value of Coefficient** - each coefficient is statistically significance (also thought as a partial F-test)\n",
    "    * **Coefficient value** - interpreting what it signifies\n",
    "        * example: average effect on $Y$ of a one unit increase in $X_2$, holding all other predictors ($X_1$ and $X_3$) fixed, is $0.4836$\n",
    "        * beware that interpretations are generally pretty hazardous due to **correlations among predictors**\n",
    "        * however, p-values for each coefficient are $\\approx$ 0, so it might be possible to interpret the coefficients\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Linear Regression Assumptions:\n",
    "* LR Assumptions - all of the linear regression model assumptions are really statements about the regression error terms, $\\epsilon$\n",
    "    1. **Linearity** - if scatterplot of residuals and y-values that follow a linear pattern (e.g. not curvilinear)\n",
    "    2. **Constant variance (homoscedasticity)** - if the scatterplot of the residuals have constant spread or variance (e.g. not in triangular form)\n",
    "    3. **Independence of errors** - are we collecting observations from the same entity over time (e.g. stock prices over time)\n",
    "        * longitudinal data - data from specific entities over time\n",
    "        * cross-sectional data - collect data on entities only once\n",
    "    4. **Normality of errors** - are the residuals in a histogram normal? is the residuals skewed?\n",
    "    5. **Lack of multicollinearity** - is there any high correlations between independent variables?\n",
    "* **Studentized Residuals (aka Standardized Residuals)** - the quotient resulting from the division of a **residual** by an estimate of its standard deviation\n",
    "<img src=http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/images/res1panel.png text=\"Studentized Residuals\" width=70% />\n",
    "    * Error terms cannot be observed directly, so we rely on the least squares *residuals*: $e_i = y_i-\\hat{y}_i$ \n",
    "    * equation: $r_i=\\frac{e_i}{s_{e_i}}=\\frac{\\epsilon_i}{\\sigma}$~$Normal(0,1)$\n",
    "    * Obtaining studentized residuals:\n",
    "        1. run the regression\n",
    "        2. calculate the predicted values\n",
    "        3. calculate the *residuals*: $e_i=y_i-\\hat{y}_i$\n",
    "        4. calculate the *studentized residuals*: $r_i=\\frac{e_i}{s_{e_i}}=\\frac{\\epsilon_i}{\\sigma}$~$Normal(0,1)$\n",
    "    * residual plot comparison types\n",
    "        * residuals $e_i$ vs. independent variables\n",
    "        * residuals $e_i$ vs. predicted/fitted values $\\hat{y}$\n",
    "* Regression diagnostics (Model Checking):\n",
    "    1. **Non-linearity with predictors**\n",
    "<img src=http://docs.statwing.com/wp-content/uploads/2014/10/Nonlinear-residual-21.png text=\"Non-linearity\" width=50% />\n",
    "        * **Polynomials**\n",
    "            * different degrees of polynomial\n",
    "        * **Step functions**\n",
    "            * easy to create and explain\n",
    "        * **Splines**\n",
    "        * **Local Regression** - using a sliding weight function to make separate linear fits over range of $X$\n",
    "        * **Generalized Additive Models (GAMs)** - adding up contributing effects\n",
    "    2. **Non-normality of Error Terms** - normality assumption allows us to construct confidence intervals and do hypothesis tests\n",
    "        * Graphical checks:\n",
    "            * **Normal Q-Q plot** - a quantile-quantile plot of the standardized data against the standard normal distribution\n",
    "<img src=https://i.stack.imgur.com/ezLDI.png text=\"Normal Q-Q Plot\" width=70% />\n",
    "            * Histogram\n",
    "        * Normality tests\n",
    "            * **Jarque-Bera test**\n",
    "            * **Shapiro-Wilk test**\n",
    "        * Solution? A log transformation of the dependent variable is often useful\n",
    "    3. **Heteroscedasticity or Non-constant Variance** - checking if the variance is constant over fitted values\n",
    "<img src=https://i.stack.imgur.com/RU17l.png text=\"Heteroscedasticity\" width=70% /> \n",
    "        * use residual plot $e_i$ vs. predicted/fitted values $\\hat{y}$\n",
    "        * Solution? transform Y via $log(Y)$ or $\\sqrt{Y}$\n",
    "    4. **Multicollinearity** - checking if features are highly correlated with each other\n",
    "        * **Correlation Matrix / Scatterplot Matrix** - shows correlation between pairwise variables\n",
    "            * downside is that it only pick up pairwise effects\n",
    "        * **Variance Inflation Factors (VIF)** - runs ordinary least squares for each predictor as function of all the other predictors\n",
    "            * equation: $VIF = \\frac{1}{1-R^2_i}$\n",
    "            * $k$ times for $k$ predictors\n",
    "            * $X_1 = \\alpha_2X_2+\\alpha_3X_3+\\cdots+\\alpha_kX_k+c_0+e$\n",
    "            * rule of thumb, $k > 10$ is problematic    \n",
    "    5. **Outliers** - occurs when $y_i$ is far from predicted, $\\hat{y}_i$\n",
    "        * may occur due to data collection, re-coding issues, dirty data, etc.\n",
    "        * least squares estimates particularly affected by outliers\n",
    "        * residual plots can help identify outliers: $e_i=y_i-\\hat{y}_i$\n",
    "            * dividing each residual by its standard error should result in \"studentized residual\", and when a value is outside this range indicates outliers\n",
    "        * different types of outliers:\n",
    "            * extreme $X$ value\n",
    "            * extreme $Y$ value\n",
    "            * extreme $X$ and $Y$\n",
    "            * disatant data point\n",
    "        * **Leverage point** - an observation with an unusual $X$ value\n",
    "<img src=leverage.png text=\"Heteroscedasticity\" width=100% />\n",
    "            * does not necessarily have a large effect on the regression model\n",
    "            * most common measure, the hat value, $h_{ii} = (H)_{ii}$\n",
    "            * the $i$th diagonal of the hat matrix: $H = X(X^TX)^{-1}X^T$\n",
    "            * high-leverage points are those observations, if any, made at extreme or outlying values of the independent variables such that the lack of neighboring observations means that the fitted regression model will pass close to that particular observation\n",
    "        * **Influential points** - an outlier that greatly affects the slope of the regression line\n",
    "<img src=influential_points.png text=\"Heteroscedasticity\" width=100% />\n",
    "            * observations that have high leverage and large residuals tend to be influential            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Categorical Variables and Interactions\n",
    "* Categorical Variables:\n",
    "    * interested in credit card balances $y$\n",
    "    * suspect it may be related to gender or ethnicity\n",
    "    * modeling with only gender\n",
    "        * $x_i = $$\\begin{cases} \n",
    "            1 & \\text{if $i$th person is female} \\\\\n",
    "            0 & \\text{if $i$th person is male}\n",
    "            \\end{cases}$\n",
    "        * $y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i = $$\\begin{cases} \n",
    "            \\beta_0+\\beta_1+\\epsilon_i & \\text{if $i$th person is female} \\\\\n",
    "            \\beta_0+\\epsilon_i & \\text{if $i$th person is male}\n",
    "            \\end{cases}$\n",
    "    * modeling with only ethnicity (more than 2 levels)\n",
    "        * $x_{i1} = $$\\begin{cases} \n",
    "            1 & \\text{if $i$th person is asian} \\\\\n",
    "            0 & \\text{if $i$th person is not asian}\n",
    "            \\end{cases}$\n",
    "        * $x_{i2} = $$\\begin{cases} \n",
    "            1 & \\text{if $i$th person is caucasian} \\\\\n",
    "            0 & \\text{if $i$th person is not caucasian}\n",
    "            \\end{cases}$\n",
    "        * $y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\epsilon_i = $$\\begin{cases} \n",
    "            \\beta_0+\\beta_1+\\epsilon_i & \\text{if $i$th person is asian} \\\\\n",
    "            \\beta_0+\\beta_2+\\epsilon_i & \\text{if $i$th person is caucasian} \\\\\n",
    "            \\beta_0+\\epsilon_i & \\text{if $i$th person is african american (AA)}\n",
    "            \\end{cases}$\n",
    "        * $\\beta_0$ as average credit card balance for AA\n",
    "        * $\\beta_1$ as difference in average balance between asian and AA\n",
    "        * $\\beta_2$ as difference in average balance between caucasian and AA\n",
    "        * recode categorical column to 0 and 1\n",
    "            * Asian: {Asian:1, Caucasian:0}\n",
    "            * Caucasian: {Asian:0, Caucasian:1}\n",
    "            * AA: {Asian:0, Caucasian:0}\n",
    "        * intercept $\\beta_0$ loses nice interpretation\n",
    "        * $\\beta_1=-23.1$ - still interpret as difference between asian and AA holding all other predictors constant (beware of interpretation)\n",
    "* **Interactions** - relationship among three or more variables\n",
    "    * example of predicting sales by TV/radio/newspaper expenditure\n",
    "        * $\\hat{sales} = \\beta_0+\\beta_1*TV+\\beta_2*radio+\\beta_3*newspaper$\n",
    "        * looks to be synergy between TV and Radio based on plot of sales vs. tv/radio\n",
    "        * account for this synergy by adding a interaction coefficient of $TV*Radio$\n",
    "        * $sales = \\beta_0+\\beta_1*TV+\\beta_2*radio+\\beta_3*(radio*TV)+\\epsilon$\n",
    "            * the coefficient estimates in the table suggest that an increase in TV advertising of \\$1,000 is associated with increased sales of $(\\hat{\\beta}_1+\\hat{\\beta}_3 * radio) * 1000 = 19+1.1 * radio$ units\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

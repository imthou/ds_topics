{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting\n",
    "\n",
    "1) Boosting Basics\n",
    "* What is Boosting?\n",
    "    * From Random Forest/Bagging: Combines set of \"weak\" learners to form strong learner (Ensemble Method)\n",
    "        * \"Weak\" in the error rate only slightly better than random guessing\n",
    "    * Difference in Boosting: Sequentially apply weak classification algorithm to modified versions of the data $\\rightarrow$ sequence of weak classifiers\n",
    "        * Uses **information** from **previous tree** to grow new tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Boosting Algorithms\n",
    "* **AdaBoost** - each tree is expert on minimizing errors of predecessor tree. Each tree will **iteratively <u>re-weight</u> observations based on errors**\n",
    "![adaboost](adaboost.png)\n",
    "    * Discrete Adaboost - AdaBoost algorithm for classification\n",
    "        * Discrete Adaboost Algorithm:\n",
    "            1. Initialize the observations weights $w_i=\\frac{1}{N}, i=1,2,\\dots,N$\n",
    "            2. For $m=1$ to $M$:\n",
    "                1. Fit a classifier $G_m(x)$ to the training data using weights $w_i$\n",
    "                2. Compute error: $err_m=\\frac{\\sum_{i=1}^N w_i I(y_i \\neq G_m(x_i))}{\\sum_{i=1}^N w_i}$\n",
    "                3. Compute weights: $\\alpha_m=log(\\frac{(1-err_m)}{err_m})$\n",
    "                4. Set $w_i \\leftarrow w_i \\cdot e^{(\\alpha_m \\cdot I(y_i \\neq G_m(x_i)))}, i=1,2,\\dots,N$\n",
    "            3. Output $G(x)=\\sum_{m=1}^M \\alpha_m G_m(x)$\n",
    "        * Adaboost Intuitive Steps:\n",
    "            1. The first weak tree, $G_1(X)$, is fit on the training data\n",
    "            2. Subsequent weak classifier trees uses same classification algorithm, but **modified weights**\n",
    "                * For previously misclassified, scale by $e^{\\alpha_m}$ else $w_i$ remains the same\n",
    "                * At each step $m$, observations previously misclassified by $G_{m-1}(x)$ have their weights increased\n",
    "                * Each successive classifier forced to **concentrate** on training observations **previously missed**\n",
    "            3. Final strong classifier, $G(x)$, determined by weighted majority votes\n",
    "                * $\\alpha_1,\\dots,\\alpha_M$ as weight of votes (gives higher influence to more accurate classifiers)\n",
    "                * The **smaller** the error of the weak classifier, the **greater** the weight\n",
    "    * Adaboost Algorithm Details:\n",
    "        * Uses **Forward Stagewise Additive Modeling** - adds new basis functions without adjusting previous parameters and coefficients\n",
    "        * Adaboot uses **Exponential Loss** Function: $L(y,f(x))=e^{(-yf(x))}$ - uses this loss function because of computational advantage\n",
    "        * Can be shown that the additive expansion in AdaBoost is estimating the function which justifies taking the sign as classification rule for final classifier\n",
    "* **Gradient Boosted** - instead of fitting to reweighted training observations, **fit <u>residuals</u> of the previous tree**\n",
    "![gradient_boost](gradient_boost.png)\n",
    "* Gradient Boosted Regression Trees Steps:\n",
    "    1. Set $\\hat{f}=0$ and $r_i=y_i$ for all $i$ in the training set\n",
    "    2. For $b=1,2,\\dots,B$ and repeat:\n",
    "        1. Fit a tree $\\hat{f}^b$ with $d$ splits $(d+1)$ terminal nodes to the training data $(X,\\gamma)$\n",
    "        2. Update $\\hat{f}$ by adding in a shrunken version of the new tree: $\\hat{f}(x)\\leftarrow \\hat{f}(x)+\\lambda\\hat{f}^b(x)$\n",
    "        3. Update the residuals: $\\gamma_i \\leftarrow \\gamma_i - \\lambda \\hat{f}^b(x_i)$\n",
    "    3. Output the boosted model: $\\hat{f}(x) = \\sum_{b=1}^B \\lambda\\hat{f}^b(x)$\n",
    "* Bias/Variance For Number of Trees Used\n",
    "![num_trees_boosted](num_trees_boosted.png)\n",
    "    * A single boosted tree - high bias, low variance\n",
    "    * A lot of boosted trees - low bias, high variance\n",
    "    * Not like random forest where having a lot of trees reduces variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Hyperparameter Tuning For Boosting - Tree Structure, Shrinkage, and Stochastic Gradient Boosting\n",
    "* **Tree Structure** - adjusting hyperparameters for depth of tree and minimal samples per leaf\n",
    "![max_depth_boosting](max_depth_boosting.png)\n",
    "    * **`max_depth`** - controls degree of interactions\n",
    "        * Example: latitude and longitude\n",
    "        * Not often larger than 4 or 6\n",
    "    * **`min_samples_per_leaf`** - may not want terminal nodes with too few leaves\n",
    "* **Shrinkage** - adjusting hyperparameters for number of trees and learning rate\n",
    "![n_estimators_and_learning_rate_boosting](n_estimators_and_learning_rate_boosting.png)\n",
    "    * **`n_estimators`** - number of trees grown\n",
    "    * **`learning_rate`** - lower learning rate requires **higher** n_estimators\n",
    "        * As the learning rate goes down, the number of trees needed goes up\n",
    "* **Stochastic Gradient Boosting** - adjusting maximium number of features per split and limiting the training set to a random subset\n",
    "![stochastic_gradient_boosting](stochastic_gradient_boosting.png)\n",
    "    * **`max_features`** - like random forest, choose a random subsample of features (great when you have too many features)\n",
    "    * **`sub_sample`** - random subset of the training set\n",
    "        * Both randomly sampling the features and randomly subseting the training set can lead to **improved accuracy** and **reduced run-time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Feature Importance For Boosting\n",
    "* For boosting regression trees, objective was to record the total amount of **RSS decreased due to splits over a given predictor, averaged over all $B$ trees** where large value indicates an important predictor\n",
    "* Similarly, for boosting classification trees, objective is to add up the **total amount that the Gini index is decreased by splits** over a given predictor, averaged over all $B$ trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) **Partial Dependence Plots** - these plots show dependence between target function, marginalizing over the values of all other features\n",
    "* e.g. proportion of spam as represented by the log-odds\n",
    "![partial_dependence_plot](partial_dependence_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "6) **eXtreme Gradient Boosting (XGBoost)**\n",
    "![xgboost](xgboost.png)\n",
    "* Scalable, Portable and Distributed Gradient Boosting library that runs on single machine, Hadoop, Spark, Flink and DataFlow\n",
    "* XGBoost provides a **parallel tree boosting** (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Streaming\n",
    "\n",
    "* Objectives:\n",
    "    * Knowing use cases for Spark Streaming\n",
    "    * How to process data via streaming\n",
    "    * Pre-existing methods for problems before Spark Streaming\n",
    "    * Understanding existing streaming systems\n",
    "    * Knowing how Spark Streaming works\n",
    "    * Knowing Spark Streaming Programming Models: Discretized Stream and Window-based Transformations\n",
    "    * Arbitrary Combinations of Batch and Streaming Computations\n",
    "    * Advantages of Unified Stack (Batch and Streaming)\n",
    "    * Advantages of Spark Streaming (Performance, Fault-tolernace, Input Sources, Output Sinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Use Cases For Spark Streaming\n",
    "* Fraud detection in bank transactions\n",
    "* Anomalies in sensor data\n",
    "* Identifying cats in videos from tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Processing, Storing, and Transforming Raw Data\n",
    "* Scales to hundreds of nodes\n",
    "* Achieves low latency\n",
    "* Efficiently recover from failures\n",
    "* Integrates with batch and interactive processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Pre-existing methods before Spark Streaming\n",
    "* Difficulties of Combined Batch and Streaming Frameworks\n",
    "    * Build two stacks - one for batch and one for streaming\n",
    "        * Often both process the same data\n",
    "    * Existing frameworks cannot do both\n",
    "        * Either, stream processing of 100s of MB/s with low latency\n",
    "        * Or, batch processing of TBs of data with high latency\n",
    "    * Extremely painful to maintain two stacks\n",
    "        * Different programming models\n",
    "        * Doubles implementation effort\n",
    "        * Doubles operational effort\n",
    "* Creating Fault-tolerant Streaming processing\n",
    "    * Traditional processing model:\n",
    "    ![mutable_stream_processing](mutable_stream_processing.png)\n",
    "        * Pipeline of nodes\n",
    "        * Each node maintains mutable state\n",
    "        * Each input record updates the state and new records are sent out\n",
    "        * Mutable state is lost if node fails\n",
    "        * Making stateful stream processing fault-tolerant is challenging\n",
    "    * Existing Streaming Systems:\n",
    "        * Apache Storm\n",
    "        ![apache_storm](https://image.slidesharecdn.com/june29550yahoopeng-160711212806/95/resource-aware-scheduling-in-apache-storm-5-638.jpg?cb=1468272514)\n",
    "            * Replays record if not processed by a node\n",
    "            * Processes each record **at least once**\n",
    "            * May update mutable state twice!\n",
    "            * Mutable state can e lost due to failure!\n",
    "        * Apache Trident\n",
    "        ![apache_trident](https://image.slidesharecdn.com/0490ef01-8e4e-4c29-b5f0-62c12f69bb97-150325220330-conversion-gate01/95/apache-storm-concepts-25-638.jpg?cb=1427321235)\n",
    "            * Processes each record **exactly once**\n",
    "            * Per-state transaction to external database is slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) What is Spark Streaming?\n",
    "![spark_streaming](spark_streaming.png)\n",
    "* Recieve data streams from input sources, process them in a cluster, push out to databases/dashboards\n",
    "* Scalable, fault-tolerant, second-scale latencies\n",
    "* Spark Streaming Steps:\n",
    "![spark_streaming_steps](spark_streaming_steps.png)\n",
    "    1. Chop up data streams into batches of few seconds\n",
    "    2. Spark treats each batch of data as RDDs and processes them using RDD operations\n",
    "    3. Processed results are pushed out in batches\n",
    "* Spark Streaming Programming Model\n",
    "    * **Discretized Stream (DStream)** - represents a stream of data and implemented as a sequence of RDDs\n",
    "        * Create input DStreams from different sources\n",
    "        * Apply parallel operations\n",
    "        * Example: Get hashtags from Twitter\n",
    "        ```scala\n",
    "        val ssc = new StreamingContext(sparkContext, Seconds(1))\n",
    "        val tweets = TwitterUtils.createStream(ssc, auth) \n",
    "        // tweets = Input DStream (stored in memory as RDDs)\n",
    "        ```\n",
    "        ![hashtags](hashtags.png)\n",
    "        ```scala\n",
    "        val hashTags = tweets.flatMap(status => getTags(status))\n",
    "        // hashTags = transformed DStream\n",
    "        // flatMap() = modifies data in one DStream to create another DStream\n",
    "        ```\n",
    "        ![hashtags_transform](hashtags_transform.png)\n",
    "        ```scala\n",
    "        hashTags.saveAsHadoopFiles(\"hdfs://...\")\n",
    "        // pushes data to external storage\n",
    "        ```\n",
    "        ![hashtags_hdfs](hashtags_hdfs.png)\n",
    "        ```scala\n",
    "        hashTags.foreachRDD(hashTagRDD => { ... })\n",
    "        // do whatever you want with the processed data\n",
    "        ```\n",
    "        ![hashtags_foreach](hashtags_foreach.png)\n",
    "        * **Window-based Transformations**\n",
    "        ```scala\n",
    "        val tagCounts = hashTags.window(Minutes(1), Seconds(5)).countByValue()\n",
    "        // window() = sliding window operation\n",
    "        // Minutes() = window length\n",
    "        // Seconds() = sliding interval\n",
    "        ```\n",
    "        ![window_transformation](window_transformation.png)\n",
    "    * **Arbitrary Stateful Computations** - specify function to generate new state based on previous state and new data\n",
    "        * Example: Maintain per-user mood as state, and update it with their tweets\n",
    "        ```scala\n",
    "        def updateMood(newTweets, lastMood) => newMood\n",
    "        val moods = tweetsByUser.updateStateByKey(updateMood _)\n",
    "        ```\n",
    "    * **Arbitrary Combinations of Batch and Streaming Computations** - intermix RDD and DStream operations\n",
    "        * Example: Join incoming tweets with a spam HDFS file to filter out bad tweets\n",
    "        ```scala\n",
    "        tweets.transform(tweetsRDD => {\n",
    "            tweetsRDD.join(spamFile).filter(...)\n",
    "        })\n",
    "        ```\n",
    "        * Combine live data streams with historical data\n",
    "            * Generate historical data models with Spark\n",
    "            * Use data models to process live data stream\n",
    "        * Combine streaming with MLlib, GraphX algorithms\n",
    "            * Offline learning, online prediction\n",
    "            * Online learning and prediction\n",
    "        * Interactively query streaming data using SQL\n",
    "        ```scala\n",
    "        spark.sql(\"select * from table_from_streaming_data\")\n",
    "        ```\n",
    "        * Advantage of an Unified Stack\n",
    "            * Explore data interactively to identify problems\n",
    "            * Use same code in Spark for processing large logs\n",
    "            * Use similar code in Spark Streaming for realtime processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Performance / Fault-tolerance / Input & Output Sources\n",
    "* **Performance** - can process 60M records/sec (6 GB/sec) on 100 nodes at sub-second latency\n",
    "![streaming_performance](streaming_performance.png)\n",
    "* **Fault-tolerance**\n",
    "    * Batches of input data are replicated in memory for fault-tolerance\n",
    "    * Data lost due to worker failure, can be recomputed from replicated input data\n",
    "    * All transformations are fault-tolerant, and **exactly-once** transformations\n",
    "    ![fault_tolerant_streaming](fault_tolerant_streaming.png)\n",
    "* **Input Sources**\n",
    "    * Natively Supported: Kafka, Flume, Kinesis, Raw TCP sockets, HDFS, etc.\n",
    "    * Easy to write customer receiver (define when receiver is started and stopped)\n",
    "    * Generate own sequence of RDDs and push them in as \"stream\"\n",
    "* **Output Sinks**\n",
    "    * HDFS, S3 (Hadoop API compatible filesystems)\n",
    "    * Cassandra (using Spark-Cassandra connector)\n",
    "    * HBase\n",
    "    * Directly push data anywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Spark Streaming vs Structured Streaming\n",
    "* (-) Prior to Spark 2.2, if you wanted to write a production streaming app you had to code against spark’s original streaming API.\n",
    "* (-) Spark Streaming API made no guarantees about out of order delivery of information and made no delivery guarantees (e.g. at least once, at most once, exactly once.) In addition, the API was very close to the classic RDD model, meaning you operated on a single micro-batch at a time and any maintenance of state across batches had to be manually handled.\n",
    "* (+) Structured Streaming API allows for \"standing queries\" over our streams, and create/update query state in a distributed database when processing every batch.\n",
    "* (+) Structured streaming API allows use of familiar DataFrame API to define standing queries that refresh their state in a consistent way. (This is very similar to what Microsoft has been doing for years with StreamInsight.)\n",
    "\n",
    "```scala\n",
    "val kafkaStream = spark.createStream(kafkaConsumer)\n",
    " \n",
    "// Process the stream and write to postgres\n",
    "kafkaStream.groupBy(col(\"organization\"), window(col(\"time\"), \"5 minute\"))\n",
    "    .sum()\n",
    "    .writeStream.format(\"jdbc\")\n",
    "    .start(\"jdbc:postgresql:…\")\n",
    "```\n",
    "* (+) Spark Streaming API works better on less structured data, and performance on less structured tasks (enrichment, security, machine learning) is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Competitors To Structured Streaming\n",
    "* **Apache Flink**\n",
    "    * (+) Has lower latency than Spark Structured Streaming\n",
    "    * Spark Structured Streaming has still microbatches used in background. However, it supports event-time processing, quite low latency (but not as low as Flink), supports SQL and type-safe queries on the streams in one API; no distinction, every Dataset can be queried both with SQL or with typesafe operators. It has end-to-end exactly-one semantics\n",
    "    * Spark Continous Processing Mode is in development and it will give Spark ~1ms latency, comparable to those from Flink. However, as I said, it's still in progress. The API is ready for non-batch jobs, so it's easier to do than in previous Spark Streaming.\n",
    "    * Spark relies on micro-batching now and Flink is has pre-scheduled operators. That means, Flink's latency is lower, but Spark Community works on Continous Processing Mode, which will work similar (as far as I understand) to receivers.\n",
    "* **Apache Beam** - is an API that separates the building of a data processing pipeline from the actual engine on which it would run. It includes abstractions for specifying the data pipeline, the actual data stream (akin to Spark's RDDs), transformation functions, \"runners\" (the compute engine), and the sources and targets. It's one of a growing number of approaches for flattening the Lambda architecture, so you can combine real time and batch processing (and interactive as well) on the same code base and cluster.\n",
    "    * (+) The biggest gap is with functions dependent on true stream processing (the ability to process one event at time and set discrete time windows), where Spark Streaming's microbatch capabilities either fall short or require workarounds. (Although Structured Streaming fixes this issue.)\n",
    "    * Each of these compute engines -- Google Cloud Dataflow, Spark, Flink, and Apex, all want to be your one-stop shop. And that's where Beam becomes coopetition with Spark -- it will work with Spark, but theoretically, it will work with other engines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) **Structured Streaming** - fast, scalable, fault-tolerant, end-to-end exactly-once stream processing **without the user having to reason about streaming** (Production Ready in Spark 2.2)\n",
    "* Overview:\n",
    "    * A scalable and fault-tolerant stream processing engine built on the Spark SQL engine \n",
    "    * Allows taking the same operations that perform in batch mode and perform the in a streaming fashion\n",
    "        * Allows for simple switches (express your streaming computation the same way you would express a batch computation on static data)\n",
    "    * Reduces latency and allow for incremental processing (The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive)\n",
    "    * The system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write Ahead Logs\n",
    "* Programming Model:\n",
    "    * The key idea in Structured Streaming is to treat a live data stream as a table that is being **continuously appended**\n",
    "        * This leads to a new stream processing model that is very similar to a batch processing model. \n",
    "    * Spark runs it as an **incremental** query on the **unbounded** input table\n",
    "    ![input_table](https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png)\n",
    "        * Consider the input data stream as the “Input Table”. \n",
    "        * Every data item that is arriving on the stream is like **a new row being appended** to the Input Table.\n",
    "    * A query on the input will generate the \"Result Table\". Every trigger interval (say, every 1 second), new rows get appended to the Input Table, which eventually updates the Result Table.\n",
    "    ![result_table](https://spark.apache.org/docs/latest/img/structured-streaming-model.png)\n",
    "        * Whenever the result table gets updated, we would want to write the changed result rows to an external sink.\n",
    "    * **Output** - defined as what gets written out to the external storage. The output can be defined in a different mode:\n",
    "        * **Complete Mode** - The entire updated Result Table will be written to the external storage. It is up to the storage connector to decide how to handle writing of the entire table.\n",
    "        * **Append Mode** - Only the new rows appended in the Result Table since the last trigger will be written to the external storage. This is applicable only on the queries where existing rows in the Result Table are not expected to change.\n",
    "        * **Update Mode** - Only the rows that were updated in the Result Table since the last trigger will be written to the external storage (available since Spark 2.1.1). Note that this is different from the Complete Mode in that this mode only outputs the rows that have changed since the last trigger. If the query doesn’t contain aggregations, it will be equivalent to Append mode.\n",
    "    * Example: Words over time\n",
    "    ![output](https://spark.apache.org/docs/latest/img/structured-streaming-example-model.png)\n",
    "        * If there is new data, Spark will run an \"incremental\" query that combines the previous running counts with the new data to compute updated counts\n",
    "        * This model is significantly different from many other stream processing engines.\n",
    "            * Many streaming systems require the user to **maintain running aggregations themselves**, thus having to reason about fault-tolerance, and data consistency (at-least-once, or at-most-once, or exactly-once).\n",
    "            * In this model, **Spark is responsible for updating the Result Table when there is new data**, thus relieving the users from reasoning about it.\n",
    "    * Handling Event-time and Late Data\n",
    "        * **Handling Event-time** - time embedded in the data itself\n",
    "            * Example: get the number of events generated by IoT devices every minute\n",
    "                * Use the time when the data was generated (that is, event-time in the data), rather than the time Spark receives them\n",
    "            * This event-time is very naturally expressed in this model – each event from the devices is a row in the table, and event-time is a column value in the row.\n",
    "            * Allows window-based aggregations (e.g. number of events every minute) to be just a special type of grouping and aggregation on the event-time column\n",
    "                * Each time window is a group and each row can belong to multiple windows/groups\n",
    "                * Such event-time-window-based aggregation queries can be defined consistently on both a static dataset (e.g. from collected device events logs) as well as on a data stream\n",
    "            * Example: Instead of running word counts, we want to count words within 10 minute windows, updating every 5 minutes\n",
    "            ![window_op_event_time](https://spark.apache.org/docs/latest/img/structured-streaming-window.png)\n",
    "                * Word counts in words received between 10 minute windows 12:00 - 12:10, 12:05 - 12:15, 12:10 - 12:20, etc.\n",
    "                * Note that 12:00 - 12:10 means data that arrived after 12:00 but before 12:10.\n",
    "                * Consider a word that was received at 12:07. This word should increment the counts corresponding to two windows 12:00 - 12:10 and 12:05 - 12:15\n",
    "        * **Handling Late Data** - this model naturally handles data that has arrived later than expected based on its event-time\n",
    "            * Since Spark is updating the Result Table, it has full control over updating old aggregates when there is late data, as well as cleaning up old aggregates to limit the size of intermediate state data\n",
    "            * Since Spark 2.1, we have support for **watermarking** which allows the user to specify the threshold of late data, and allows the engine to accordingly clean up old state\n",
    "            * Example: Text Over Time with Late Data\n",
    "            ![window_op_late_data](https://spark.apache.org/docs/latest/img/structured-streaming-late-data.png)\n",
    "                * A word generated at 12:04 (i.e. event time) could be received by the application at 12:11\n",
    "                * The application should use the time 12:04 instead of 12:11 to update the older counts for the window 12:00 - 12:10\n",
    "                * It’s necessary for the system to bound the amount of intermediate in-memory state it accumulates (means the system needs to know when an old aggregate can be dropped from the in-memory state because the application is not going to receive late data for that aggregate any more)\n",
    "                * **Watermarking** - lets the engine automatically track the current event time in the data and attempt to clean up old state accordingly\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Example For Structured Streaming\n",
    "* Example: Retail Store Dataset (Pay attention to code changes)\n",
    "    * Read as **static** data using DataFrame API\n",
    "    ```scala\n",
    "    %scala\n",
    "    val staticDataFrame = spark.read.format(“csv”)\n",
    "       .option(“header”, “true”)\n",
    "       .option(“inferSchema”, “true”)\n",
    "       .load(\"dbfs:/mnt/defg/retail-data/by-day/*.csv\")\n",
    "    staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
    "    val staticSchema = staticDataFrame.schema\n",
    "    ```\n",
    "    ```python\n",
    "    %python\n",
    "    staticDataFrame = spark.read.format(\"csv\") \\\n",
    "       .option(\"header\", \"true\") \\\n",
    "       .option(\"inferSchema\", \"true\") \\\n",
    "       .load(\"dbfs:/mnt/defg/retail-data/by-day/*.csv\")\n",
    "    staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
    "    staticSchema = staticDataFrame.schema\n",
    "    ```\n",
    "    * Aggregating static time-series data by largest sale hours where a given customer (`CustomerId`) makes a large purchase (total cost column and days a customer spent the most)\n",
    "    ```scala\n",
    "    %scala\n",
    "    import org.apache.spark.sql.functions.{window, column, desc, col}\n",
    "    staticDataFrame.selectExpr(\n",
    "       \"CustomerId\",\n",
    "       \"(UnitPrice * Quantity) as total_cost\",\n",
    "       \"InvoiceDate\")\n",
    "       .groupBy(col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\n",
    "       .sum(\"total_cost\")\n",
    "       .orderBy(desc(\"sum(total_cost)\"))\n",
    "       .take(5)\n",
    "    ```\n",
    "    ```python\n",
    "    %python\n",
    "    from pyspark.sql.functions import window, column, desc, col\n",
    "    staticDataFrame.selectExpr(\n",
    "        \"CustomerId\",\n",
    "        \"(UnitPrice * Quantity) as total_cost\", \n",
    "        \"InvoiceDate\") \\\n",
    "        .groupBy(col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\")) \\\n",
    "        .sum(\"total_cost\") \\\n",
    "        .orderBy(desc(\"sum(total_cost)\")) \\\n",
    "        .take(5)\n",
    "    ```\n",
    "    ```sql\n",
    "    %sql\n",
    "    SELECT\n",
    "       sum(total_cost),\n",
    "       CustomerId,\n",
    "       to_date(InvoiceDate)\n",
    "    FROM (SELECT\n",
    "       CustomerId,\n",
    "       (UnitPrice * Quantity) as total_cost,\n",
    "       InvoiceDate\n",
    "    FROM\n",
    "       retail_data)\n",
    "    GROUP BY\n",
    "       CustomerId, to_date(InvoiceDate)\n",
    "    ORDER BY\n",
    "       sum(total_cost) DESC\n",
    "    ```\n",
    "    * Read as **streaming** data using DataFrame API\n",
    "        * Use `readStream` instead of `read`\n",
    "        * `maxFilesPerTrigger` - specifies number of files we should read in at once\n",
    "            * In production scenario, this would be omitted\n",
    "        * More executors the better for high number of partitions (by default is 200 for DataFrame API)\n",
    "    ```scala\n",
    "    %scala\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\") // less partitions for local mode with less executors\n",
    "    val streamingDataFrame = spark.readStream\n",
    "        .schema(staticSchema)\n",
    "        .option(\"maxFilesPerTrigger\", 1)\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(\"dbfs:/mnt/defg/retail-data/by-day/*.csv\")\n",
    "    streamingDataFrame.isStreaming // returns true \n",
    "    ```\n",
    "    ```python\n",
    "    %python\n",
    "    streamingDataFrame = spark.readStream \\\n",
    "        .schema(staticSchema) \\\n",
    "        .option(\"maxFilesPerTrigger\", 1) \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .load(\"dbfs:/mnt/defg/retail-data/by-day/*.csv\")\n",
    "    streamingDataFrame.isStreaming() # returns true\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality Reduction\n",
    "\n",
    "* Objectives:\n",
    "    * Why reduce dimensionality?\n",
    "    * Methods for reducing dimensionality\n",
    "    * One common technique: Principal Components Analysis (PCA)\n",
    "    * Crude facial recognition with PCA and kNN (e.g. Eigenfaces)\n",
    "    * Singular Value Decomposition (SVD)\n",
    "    * SVD vs. PCA\n",
    "    * SVD for capturing latent features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) The Importance of Reducing Dimensionality\n",
    "* Dimensionality = number of features = number of predictors\n",
    "* Why reduce dimensionality?\n",
    "    * High dimensional data causes many problems\n",
    "    * **Curse of Dimensionality** - points are \"far away\" in high dimensions, and it's **easy to overfit** small datasets (sparsity of sample data points)\n",
    "        * **Distance between nearest neighbors is very large**\n",
    "        * Stimulation: Uniformly distributed points in a cube\n",
    "        ![distance_in_hyperspace](distance_in_hyperspace.png)\n",
    "        * Stimulation: Uniformly distributed points in a hypercube (most data points are **closer to the boundary of the sample space**)\n",
    "        ![hypercube](hypercube.png)\n",
    "        * **Sample sparsity** - when the dimensionality $d$ increases, the volume of the space increases so fast that **the available data becomes sparse**\n",
    "            * Example: Uniformly distributed data points ($N=1000$) and cover 20% of range of each space\n",
    "            ![sample_sparsity](sample_sparsity.png)\n",
    "            * In **ten dimensions**, we need to **cover 80% of the range** (`e(r)`) of each coordinate to **capture 10% of the data** (`fraction of volume`)\n",
    "            ![10_dimensions](10_dimensions.png)\n",
    "                * For a fraction $r$ of unit volume: $$\\text{Edge Length: } e(r)=r^{\\frac{1}{d}}$$\n",
    "                * Reducing $r$ gives fewer observations to average, and higher variance of fit\n",
    "        * **Classifier Performance** - as the dimensionality increases, the classifier's performance increases until the optimal number of features is reached\n",
    "        ![classifier_performance](classifier_performance.png)\n",
    "            * **Increasing the <u>dimensionality</u>** further **without increasing the number of training <u>samples</u>** results in a **decrease in classifier <u>performance</u>**\n",
    "    * **Difficult Visualization** - it's hard visualize anything more than 3 dimensions\n",
    "    * **Finding Latent Features** - often the most relevant features are not explicitly present in the raw high dimensional data (especially for image/video data)\n",
    "    * **Removing Correlation** - with many, many features (dimensions), there will most likely be a lot of correlations (e.g. consider neighboring pixels in an image dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Other Methods For Reducing Dimensionality\n",
    "* Subset Selection of Features (e.g. **Forward Stepwise Selection**) - selecting the best model with the best subset of features based on some metric (e.g. Mallow's C, AIC, BIC, Adjusted $R^2$, or cross-validation)\n",
    "* **Lasso Regression** - creates sparse models by zeroing out features that less importance\n",
    "* **Relaxed Lasso**\n",
    "    1. Using cross-validation and lasso regression, find the best value for $\\lambda$\n",
    "    2. Keep only the features with non-zero coefficients\n",
    "    3. Re-fit using ordinary least squares (OLS) regression (which is refitting using no regularization)\n",
    "* **Upper-layer Features** (in NN for labeled training data) - train the neural network, then interpret the output of the hidden neurons (fully-connected layer before output) as high-level features\n",
    "* **Autoencoders** (in NN for labeled or unlabeled training data) - autoencoders are neural networks that has the network learn to **reconstruct the input** instead of learning the target\n",
    "![autoencoders](autoencoders.png)\n",
    "    * Force the information through a **bottleneck hidden layer**\n",
    "    * Converges on PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **Principal Components Analysis (PCA)** - common dimensionality reduction technique that **doesn't require labeled data**\n",
    "* First goal of PCA is to **remove <u>correlation</u> between features**\n",
    "* A side effect is that we can use the process to **reduce the <u>dimensionality</u>** of the data while **preserving most of the <u>variance</u> in our data**\n",
    "* **Rank and Dimensionality**\n",
    "    * $\\mathbf{x}_i^T$ are dimensional row vectors (feature space)\n",
    "    * $\\mathbf{y}_j$ are column vectors (dependent/target space)\n",
    "    * **Linear Independence**:\n",
    "        * For any set of $n$ vectors, a **linear combination** is an expression of the form: $c_1\\mathbf{y}_1+c_2\\mathbf{y}_2+\\cdots+c_n\\mathbf{y}_n=0$\n",
    "            * If this equation holds only if all $c_i$'s are zero, then $y_i$'s are **linear independent** vectors\n",
    "            * If this equation holds with $c_i$'s **not equal** to zero, then $y_i$'s are **linear dependent** vectors. This means we can **express at least one of the vectors as linear combination of the others**: $\\mathbf{y}_1=k_2\\mathbf{y}_2+\\cdots+k_n\\mathbf{y}_n$ where $k_j=\\frac{-c_j}{c_1}$\n",
    "    * Rank of Matrix $\\mathbf{D}$:\n",
    "        * **Maximum** number of linearly independent **column** vectors of $\\mathbf{D}$\n",
    "        * **Maximum** number of linearly independent **row** vectors of $\\mathbf{D}$\n",
    "        * $\\mathbf{D}=\\left[\\begin{array}{cc}\n",
    "            \\mathbf{y}_1 & \\mathbf{y}_2 & \\cdots & \\mathbf{y}_d\n",
    "            \\end{array}\\right]$\n",
    "            * $\\mathbf{D}$ is a $[n \\times d]$\n",
    "            * $rank(\\mathbf{D})=r\\leq min(n,d)$\n",
    "        * Rank of data matrix gives the dimensionality of the data\n",
    "    * The **number of linearly independent <u>basis vectors</u>** needed to represent a **data vector**, gives the **dimensionality of the data**\n",
    "        * Equation: $\\mathbf{x}=x_1\\mathbf{e}_1+x_2\\mathbf{e}_2+\\cdots+x_d\\mathbf{e}_d$\n",
    "        ![basis_vectors](basis_vectors.png)\n",
    "        * The data points apparently reside in a $d$-dimensional **attribute space**\n",
    "        * But, if $r<d$, then the data points actually reside in a lower $r$-dimensional space\n",
    "    * Determining vector space for orthonormal basis vectors:\n",
    "        ![orthonormal_basis_vectors](orthonormal_basis_vectors.png)\n",
    "        * $\\mathbf{D}=\\left[\\begin{array}{cc}\n",
    "            \\mathbf{x}_1^T \\\\\n",
    "            \\mathbf{x}_2^T \\\\\n",
    "            \\vdots \\\\\n",
    "            \\mathbf{x}_n^T\n",
    "            \\end{array}\\right]$\n",
    "        * Each point $\\mathbf{x}_i^T=(x_1,x_2,\\dots,x_d)^T$ is a vector in $d$-dimensional vector space\n",
    "        * Write $\\mathbf{x}$ as: $\\mathbf{x}=\\sum_{i=1}^d x_i\\mathbf{e}_i$ where $e_i$ are **orthonormal basis vectors**:\n",
    "            * $\\mathbf{e}_i^Te_j=1$ if $i=j$\n",
    "            * $\\mathbf{e}_i^Te_j=0$ if $i\\neq j$\n",
    "    * Changing the dimensional space to fit along the data vector space\n",
    "        ![orthonormal_basis_vectors_2](orthonormal_basis_vectors_2.png)\n",
    "        * $\\mathbf{D}=\\left[\\begin{array}{cc}\n",
    "            \\mathbf{x}_1^T \\\\\n",
    "            \\mathbf{x}_2^T \\\\\n",
    "            \\vdots \\\\\n",
    "            \\mathbf{x}_n^T\n",
    "            \\end{array}\\right]$\n",
    "        * Each point $\\mathbf{x}_i^T=(x_1,x_2,\\dots,x_d)^T$ is a vector in $d$-dimensional vector space\n",
    "        * Given any other set of $d$ orthonormal vectors: $\\mathbf{x}$ as: $\\mathbf{x}=\\sum_{i=1}^d a_i\\mathbf{u}_i$ where $u_i$ are **orthonormal basis vectors**:\n",
    "            * $\\mathbf{u}_i^Tu_j=1$ if $i=j$\n",
    "            * $\\mathbf{u}_i^Tu_j=0$ if $i\\neq j$  \n",
    "        * $\\mathbf{A}=\\left[\\begin{array}{cc}\n",
    "            \\mathbf{a}_1^T \\\\\n",
    "            \\mathbf{a}_2^T \\\\\n",
    "            \\vdots \\\\\n",
    "            \\mathbf{a}_n^T\n",
    "            \\end{array}\\right]$\n",
    "        * Each point $\\mathbf{a}_i^T=(a_1,a_2,\\dots,a_d)^T$ is a vector in $d$-dimensional vector space\n",
    "        * $a_j=\\mathbf{u}_j^T\\mathbf{x}$\n",
    "        * In vector form:\n",
    "            $$\\mathbf{a}=\\mathbf{U}^T\\mathbf{x}$$\n",
    "            $$\\mathbf{U}=(\\mathbf{u}_1,\\mathbf{u}_2,\\dots,\\mathbf{u}_d)$$\n",
    "    * Finding the optimal set of **orthonormal basis vectors**:\n",
    "        * Because there are potentially infinite choices for the set of orthonormal basis vectors, one natural question is whether there exists an **optimal** basis, for a suitable notion of optimality\n",
    "        * Finding a reduced dimensionality subspace that still preserves the essential characteristics of the data (high variance explained)\n",
    "        * Project data points from a $d$-dimensional space to an $r$-dimensional space where $r<d$\n",
    "        $$\\mathbf{x}'=\\sum_{i=1}^r a_i \\mathbf{u}_i$$\n",
    "        $$\\mathbf{\\epsilon}=\\sum_{i=1}^d a_i \\mathbf{u}_i = \\mathbf{x}-\\mathbf{x}'$$ <center>(error vector)</center>\n",
    "* PCA basic terminology:\n",
    "    * **Principle Component Analysis (PCA)** is a technique that seeks a **$r$-dimensional basis that best captures the variance in the data**\n",
    "        * **First Principal Component** - the direction with the **largest projected variance**\n",
    "        * **Second Principal Component** - the orthogonal direction that captures the **second largest projected variance**\n",
    "    ![principal_component](principal_component.png)\n",
    "    ![pca_firsttwo](pca_firsttwo.png)\n",
    "* First Principle Component calculation:\n",
    "    * Choose the direction $\\mathbf{u}$ such that the variance of the projected points is maximized\n",
    "    * The projected variance along $\\mathbf{u}$ is: $$\\sigma_u^2=\\frac{1}{n}\\sum_{i=1}^n (a_i-\\mu_{\\mathbf{u}})^2$$\n",
    "    * For centered data: $$\\sigma_u^2=\\mathbf{u}^T\\mathbf{\\Sigma}\\mathbf{u}$$\n",
    "    * $\\mathbf{\\Sigma}$ is covariance matrix of centered $\\mathbf{D}$: $$\\mathbf{\\Sigma}=\\frac{1}{n}\\mathbf{D}^T\\mathbf{D}$$\n",
    "    * **Maximizing variance, $\\sigma$,** (with constraint $\\mathbf{u}^T\\mathbf{u}=1$) gives:\n",
    "        $$\\mathbf{\\Sigma}\\mathbf{u}=\\mathbf{\\lambda}\\mathbf{u}$$\n",
    "        $$\\sigma_{\\mathbf{u}}^2=\\mathbf{\\lambda}$$\n",
    "        * To maximize projected variance, maximize the eigenvalue of $\\mathbf{\\Sigma}$\n",
    "        * Eigenvector $\\mathbf{u}$ with **maximum $\\mathbf{\\lambda}$** specifies the direction of most variance (First Principal Component)\n",
    "* Finding all eigenvectors corresponding to $\\lambda$:\n",
    "    * To find the best $r$-dim approximation to $\\mathbf{D}$, compute the eigenvalues of the covariance matrix $\\mathbf{\\Sigma}$\n",
    "    * Eigenvalues of $\\mathbf{\\Sigma}$ are non-negative, and be sorted in decreasing order: $$\\lambda_1\\geq \\lambda_2\\geq \\cdots \\lambda_r \\geq \\lambda_{r+1} \\cdots \\geq \\lambda_d \\geq 0$$\n",
    "        * Eigenvector corresponding to $\\lambda_1$ gives first principle component\n",
    "        * Eigenvector corresponding to $\\lambda_2$ gives second principle component\n",
    "* Projected variance and minimizing MSE:\n",
    "    * Reduced $r$-dimensional data matrix: $$\\mathbf{A}=[\\mathbf{\\alpha}_1,\\mathbf{\\alpha}_2,\\cdots,\\mathbf{\\alpha}_r]$$\n",
    "    * Total variance of $\\mathbf{A}$: $$var(\\mathbf{A})=\\sum_{i=1}^r \\lambda_i$$\n",
    "    * The first $r$-principal components maximize the projected variance, $var(\\mathbf{A})$, and thus minimize the MSE:\n",
    "        $$MSE=\\frac{1}{n}\\sum_{i=1}^n \\mathbf{\\epsilon}_i^T\\mathbf{\\epsilon}_i=var(\\mathbf{D})-var(\\mathbf{A})$$\n",
    "        $$\\mathbf{\\epsilon}_i = \\mathbf{x}_i-\\mathbf{x}_i'$$\n",
    "* **Choosing the dimensionality** - how many dimensions, $r$, to use for a good approximation\n",
    "    * Compute the fraction of the total variance captured by the first $r$ principal components: $$f(r)=\\frac{\\lambda_1+\\lambda_2+\\cdots+\\lambda_r}{\\lambda_1+\\lambda_2+\\cdots+\\lambda_d}=\\frac{var(\\mathbf{A})}{var(\\mathbf{D})}$$\n",
    "    * Starting from the first principal component, then keep on adding additional components, and stop at the smallest value $r$, for which $f(r)\\geq\\alpha$\n",
    "    * In practice, $\\alpha$ is usually set to 0.9 or higher, so that the reduced dataset captures at least 90% of the total variance\n",
    "* PCA for $n<d$:\n",
    "    * The covariance matrix is a $d\\times d$ matrix\n",
    "    * Typical algorithms for finding eigenvectors of $d \\times d$ matrix have a computational cost that scales like $O(d^3)$\n",
    "    * For $n<d$, there are only $n$ non-zero eigenvalues of covariance matrix\n",
    "    * Starting from the eigenvalue equation: \n",
    "        $$\\mathbf{\\Sigma}\\mathbf{u}_i=\\mathbf{\\lambda}_i\\mathbf{u}_i$$\n",
    "        $$\\frac{1}{n}\\mathbf{D}\\mathbf{D}^T\\mathbf{v}_i=\\mathbf{\\lambda}_i\\mathbf{v}_i$$\n",
    "        $$\\text{where }\\mathbf{v}_i=\\mathbf{D}\\mathbf{u}_i$$\n",
    "    * Converted to eigenvalue equation of $n \\times n$ matrix:\n",
    "        $$\\mathbf{u}_i=\\frac{1}{\\sqrt{n\\lambda_i}}\\mathbf{D}^T\\mathbf{v}_i$$\n",
    "    * Summary of steps:\n",
    "        1. Evaluate $\\mathbf{D}\\mathbf{D}^T$\n",
    "        2. Find its eigenvectors and eigenvalues\n",
    "        3. Compute the eigenvectors in the original data space\n",
    "* **Summary of PCA Steps**:\n",
    "    1. Create the **centered design matrix** ($M$) - center the data by subtracting the mean. Transform centered data into matrix form where each row is one example\n",
    "    2. Compute the **covariance matrix** ($M^TM$)\n",
    "    3. Principal components are the **eigenvectors** of the covariance matrix - order the eigenvectors by decreasing corresponding eigenvalues to get an uncorrelated and orthogonal basis vector capturing the directions of most-to-least variance in your data\n",
    "        * **Eigenvector** - size of each eigenvalue denotes the amount of variance captured by that eigenvector\n",
    "* Example: Yale Face Database (using a subset of 105 face images)\n",
    "    * Each image is 320x240 pixels, grayscale, centered, and cropped identically\n",
    "    ![face_images](face_images.png)\n",
    "    * **Eigenfaces** - result of applying PCA on images and looking at the eigenvectors of the face database covariance matrix:\n",
    "    ![eigenfaces](eigenfaces.png)\n",
    "        * What is each eigenface capturing?\n",
    "    * Drawbacks of Eigenfaces: (Using the old method in 1987-1991)\n",
    "        * (-) Faces must be aligned eyes to eyes, mouth to mouth - differences in translation and scale are captured by PCA (which isn't what we want)\n",
    "        * (-) Faces must be lit the same - differences in lighting are captured by PCA (which isn't what we want)\n",
    "    * Improvements to methodology:\n",
    "        * **Fisherfaces** - uses LDA and labels to help remove lighting effects\n",
    "        * **Active Shape Model** - use PCA on shapes detected in the image\n",
    "    * Crude Facial Recognition via PCA and kNN - using PCA on cropped face images (aka Eigenfaces) and combined with kNN, the model yields a rough facial recognition system\n",
    "    ![pca_knn](pca_knn.png)\n",
    "* Example: MNIST dataset (dataset of handwritten digits, 10 classes (0-9), 28x28 pixels (yielding 784-dimensional vector space), grayscale, 60,000 training images / 10,000 test images)\n",
    "    * Each eigenvector of the covariance matrix is a vector in the original $d$-dimensional space that can be represented as images of the same size as the data points\n",
    "    ![lambda_eigenvectors](lambda_eigenvectors.png)\n",
    "    * What are the sizes of the eigenvalues? (recall that the size of each eigenvector's eigenvalue denotes the amount of variance captured by that eigenvector)\n",
    "        * Plot of the complete spectrum of eigenvalues, sorted into decreasing order\n",
    "    ![eigenvalues](eigenvalues.png)\n",
    "    * Reconstructing the input by a linear combination of eigenvectors:\n",
    "    ![comb_eigenvectors](comb_eigenvectors.png)\n",
    "    * Embedding in 2D:\n",
    "    ![embedding_2d](embedding_2d.png)\n",
    "* When to use PCA (generally):\n",
    "    * kNN on high dimensional data\n",
    "    * Clustering high dimensional data\n",
    "    * Visualization (e.g. embeddings)\n",
    "    * Working with images (e.g. feeding an image into a decision tree model)\n",
    "* When **not** to use PCA:\n",
    "    * Retain interpretability of the feature space\n",
    "    * Model doesn't need reduced dimensional data (e.g. OLS on relatively small dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Singular Value Decomposition (SVD) - more generalized matrix decomposition\n",
    "* **Singular Value Decomposition** - method of decomposing a matrix into three other matrices\n",
    "    * Factorization of a real or complex matrix. It is the generalization of the eigen decomposition of a positive semidefinite normal matrix to any $d \\times n$\n",
    "    * An $n \\times d$ data matrix $\\mathbf{D}$ can be factorized as: \n",
    "        * $\\mathbf{D}=\\mathbf{L}\\mathbf{\\Delta}\\mathbf{R}^T$\n",
    "            * $\\mathbf{L} = (n \\times n)$ left singular matrix\n",
    "            * $\\mathbf{R} = (d \\times d)$ right singular matrix\n",
    "            * $\\mathbf{\\Delta} = (n \\times d)$ diagonal matrix\n",
    "        * $A = USV^T$\n",
    "            * $A$ is an `(m x n)` matrix\n",
    "            * $U$ is an `(m x m)` orthogonal matrix (left singular vectors)\n",
    "            * $S$ is an `(m x n)` non-negative rectangular diagonal matrix (singular values)\n",
    "            * $V$ is an `(n x n)` orthogonal matrix (right singular vectors)\n",
    "    * $\\mathbf{\\Delta}(i,j) = $$\\begin{cases} \n",
    "            \\delta_i & \\text{if } i=j \\\\\n",
    "            0 & \\text{if } i\\neq j \n",
    "            \\end{cases}$\n",
    "        * $i=1,\\dots,n$\n",
    "        * $j=1,\\dots,d$\n",
    "        * $\\delta_i$ = singular values\n",
    "    * If the rank of $\\mathbf{D}$ is $r \\leq min(n,d)$, then there will be only $r$ non-zero singular values: \n",
    "        * $\\delta_1 \\geq \\delta_2\\geq \\cdots \\geq \\delta_r \\geq 0$\n",
    "    * Can discard those **left** and **right** singular vectors that correspond to zero singular values, to obtain the reduced **SVD** as: \n",
    "        * $\\mathbf{D}_r=\\mathbf{L}_r\\mathbf{\\Delta}_r\\mathbf{R}_r^T$\n",
    "        ![svd](svd.png)\n",
    "* SVD vs PCA:\n",
    "![pca_vs_svd](pca_vs_svd.png)\n",
    "    * PCA is a **special case** of more general matrix decomposition (SVD)\n",
    "    * PCA yields the following decomposition of covariance matrix:\n",
    "        * $\\mathbf{\\Sigma}=\\mathbf{U}\\mathbf{\\Lambda} \\mathbf{U}^T$\n",
    "        * Covariance matrix in new basis: $\\mathbf{\\Lambda}=\\left[\\begin{array}{cc}\n",
    "            \\mathbf{\\lambda}_1 & 0 & \\cdots & 0 \\\\\n",
    "            0 & \\mathbf{\\lambda}_2 & \\cdots & 0 \\\\\n",
    "            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            0 & 0 & \\cdots & \\mathbf{\\lambda}_d \\\\\n",
    "            \\end{array}\\right]$\n",
    "        * $\\mathbf{\\sigma}_i^2=\\mathbf{\\lambda}_i=\\mathbf{u}_i^T\\mathbf{\\Sigma}\\mathbf{u}_i$\n",
    "    * Matrix comparison:\n",
    "        * PCA gives: $\\mathbf{D}^T\\mathbf{D}=n\\mathbf{\\Sigma}=\\mathbf{U}(n\\mathbf{\\Lambda})\\mathbf{U}^T$\n",
    "        * SVD gives: $\\mathbf{D}^T\\mathbf{D}=(\\mathbf{R}\\mathbf{\\Delta}\\mathbf{L}^T)^T(\\mathbf{R}\\mathbf{\\Delta}\\mathbf{L}^T)=\\mathbf{R}\\mathbf{\\Delta}_d^2\\mathbf{R}^T$\n",
    "            * where $\\mathbf{\\Delta}_d^2$ is the ($d\\times d$) diagonal matrix defined as: $\\mathbf{\\Delta}_d^2(i,i)=\\delta_i^2$\n",
    "        * Comparing both: $\\delta_i^2=n\\lambda_i$, $\\mathbf{R}=\\mathbf{U}$\n",
    "    * Singular vector comparison:\n",
    "        * The right singular vectors in $\\mathbf{R}$ are the same as eigenvectors of $\\mathbf{\\Sigma}$\n",
    "        * The left singular vectors in $\\mathbf{L}$ are the eigenvectors of the matrix ($n \\times n$), matrix $\\mathbf{D}\\mathbf{D}^T$, and the corresponding eigenvalues are given as $\\delta_i^2$ \n",
    "* Capturing Latent Features in SVD (Finding Topics/Concepts)\n",
    "    * SVD can be used to find latent topics in the data\n",
    "    * Example 1: People and Food Preferences Matrix ($\\mathbf{D}=\\mathbf{L}\\mathbf{\\Delta}\\mathbf{R}^T$ or $A = USV^T$)\n",
    "        * $A$ or $\\mathbf{D}=$\n",
    "        ![latent_topics](latent_topics.png)\n",
    "        * $S$ or $\\mathbf{\\Delta}=$\n",
    "        ```python\n",
    "        array([[ 8.45, 0.  , 0.  , 0.  , 0.  ],\n",
    "               [ 0.  , 6.98, 0.  , 0.  , 0.  ],\n",
    "               [ 0.  , 0.  , 1.83, 0.  , 0.  ],\n",
    "               [ 0.  , 0.  , 0.  , 1.5 , 0.  ],\n",
    "               [ 0.  , 0.  , 0.  , 0.  , 1.1 ],\n",
    "               [ 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
    "               [ 0.  , 0.  , 0.  , 0.  , 0.  ]])\n",
    "        ```\n",
    "        * $V^T$ or $\\mathbf{R}^T=$\n",
    "        ![latent_topics_rt](latent_topics_rt.png)\n",
    "        * $U$ or $\\mathbf{L}=$\n",
    "        ![latent_topics_r2](latent_topics_r2.png)\n",
    "    * Example 2: Movie and User Preferences Matrix\n",
    "        ![a_svd](a_svd.png)\n",
    "        * $A$ matrix:\n",
    "            * The columns represent movies (Avengers, StarWars, IronMan, Titanic, Notebook)\n",
    "                * \"SciFi\" genre - Avengers, StarWars, IronMan\n",
    "                * \"Romance\" genre - Titanic, Notebook\n",
    "            * The rows are different user ratings on a scale of 0 to 5\n",
    "        ![u_svd](u_svd.png)\n",
    "        * $U$ matrix:\n",
    "            * First column represents weights that would match each user's preference to movies categorized under \"SciFi\"\n",
    "            * Second column represents weights that would match each user's perference to movies under \"Romance\" category\n",
    "            * e.g. First user greatly prefers SciFi movies (0.13 score) vs. Romance movies (0.02 score)\n",
    "        ![s_svd](s_svd.png)\n",
    "        * $S$ matrix:\n",
    "            * The first diagonal entry represents the weight of the SciFi category\n",
    "            * The second diagonal entry represents the weight of the Romance category\n",
    "            * The third diagonal entry which represents the weight of a movie category has a small value (1.3 score). This makes sense since there are only two categories of movies, so the third dimension is just considered noise\n",
    "        ![vt_svd](vt_svd.png)\n",
    "        * $V^T$ matrix:\n",
    "            * The columns depict the degree to which a movie belongs to a category\n",
    "            * e.g. The first column of $V$ that the first movie (Avengers) belongs heavily to the SciFi category (0.56 score) and very little to the Romance category (0.12 score)\n",
    "* **Multidimensional Scaling (MDS)** - a process used to find lower dimensional representaion that give that same distance between points\n",
    "    * distance matrix or proximity matrix:\n",
    "        * $\\mathbf{\\Delta}=\\left[\\begin{array}{cc}\n",
    "            \\mathbf{\\delta}_{11} & \\mathbf{\\delta}_{12} & \\cdots & \\mathbf{\\delta}_{1n} \\\\\n",
    "            \\mathbf{\\delta}_{21} & \\mathbf{\\delta}_{22} & \\cdots & \\mathbf{\\delta}_{2n} \\\\\n",
    "            \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            \\mathbf{\\delta}_{n1} & \\mathbf{\\delta}_{n2} & \\cdots & \\mathbf{\\delta}_{nn} \\\\\n",
    "            \\end{array}\\right]$\n",
    "        * $\\mathbf{\\delta}_{ij}=\\Vert \\mathbf{x}_i-\\mathbf{x}_j\\Vert$ (Euclidean distance between two vectors)\n",
    "    * MDS algorithm steps:\n",
    "        1. Calculate the matrix of squared proximities: $\\mathbf{\\Delta}^2$\n",
    "        2. Calculate the matrix: $\\mathbf{B}=\\frac{1}{2}\\mathbf{J}\\mathbf{\\Delta}^2\\mathbf{J}$, $\\mathbf{J}=\\mathbf{I}-\\frac{1}{n}\\mathbf{1}\\mathbf{1}^T$\n",
    "        3. Obtain SVD of $\\mathbf{B}$: $\\mathbf{B}=\\mathbf{U}\\mathbf{\\Lambda}\\mathbf{U}^T$\n",
    "    * For a 2-dimensional representation, keep two eigenvectors corresponding to the largest eigenvalues:\n",
    "    ![mds](mds.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "5) **t-Distributed Stochastic Neighbor Embedding (t-SNE)** - a nonlinear dimensionality reduction technique that is particularly well-suited for embedding high-dimensional data into a space of two or three dimensions, which can then be visualized in a scatter plot\n",
    "* Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points\n",
    "    * It is often used to visualize high-level representations learned by an artificial neural network\n",
    "    ![t_sne_digits](http://2.bp.blogspot.com/--l8yNRipldU/Vg5ECxalQmI/AAAAAAAAAws/s1rDpWuvtaY/s1600/tsne.png)\n",
    "* t-SNE algorithm comprises two main stages:\n",
    "    1. t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked, whilst dissimilar points have an extremely small probability of being picked\n",
    "    2. t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the **Kullbackâ€“Leibler (KL) divergence** between the two distributions with respect to the locations of the points in the map\n",
    "        * Note that whilst the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this should be changed as appropriate\n",
    "* t-SNE algorithm:\n",
    "    * Given a set of $N$ high-dimensional objects $x_1,\\dots,x_N$, t-SNE first computes probabilities $p_{ij}$ that are proportional to the similarity of the objects $x_i$ and $x_j$: $$p_{j\\mid i} = \\frac{e^{\\big(\\frac{-\\Vert x_i - x_j \\Vert^2}{2\\sigma^2_i}\\big)}}{\\sum_{k \\neq i}e^{\\big(\\frac{-\\Vert x_i - x_k \\Vert^2}{2\\sigma^2_i}\\big)}}$$\n",
    "        * The similarity of datapoint $x_j$ to datapoint $x_i$ is the conditional probability, $p_{j \\mid i}$, that $x_i$ would pick $x_j$ as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian centered at $x_i$: $$p_{ij}=\\frac{p_{j \\mid i} + p_{i \\mid j}}{2N}$$\n",
    "    * t-SNE aims to learn a $d$-dimensional map $y_1,\\dots,y_N$ (with $y_i \\in \\mathbb{R}^d$ that reflects the similarities $p_{ij}$ as well as possible.\n",
    "        * To this end, it measure similarities $q_{ij}$ between two points in the map $y_i$ and $y_j$, using very similar approach: $$q_{ij} = \\frac{\\frac{1}{1+\\Vert y_i-y_j \\Vert^2}}{\\sum_{k \\neq i} \\frac{1}{1+\\Vert y_i-y_k \\Vert^2}}$$\n",
    "        * Herein a heavy-tailed **Student-t distribution** (with one-degree of freedom) is used to measure similarities between low-dimensional points in order to allow dissimilar objects to be modeled far apart in the map\n",
    "* t-SNE has been used in a wide range of applications:\n",
    "    * computer security research\n",
    "    * music analysis\n",
    "    * cancer research\n",
    "    * bioinformatics\n",
    "    * biomedical signal processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

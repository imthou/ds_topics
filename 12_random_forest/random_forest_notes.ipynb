{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests (Ensemble Method)\n",
    "\n",
    "1) **Bagging (<u>B</u>ootstrap <u>Agg</u>regat<u>ing</u>)** - alternative method (**post-pruning**) to reducing variance in decision trees by **growing as many large trees** and then **averaging away the variance**\n",
    "* **Ensemble Method** - combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator (e.g. several base decision trees to get random forest)\n",
    "    1. **Bootstrapping** (For Decision Trees) - **random sampling with replacement of dataset** to create multiple decision tree models\n",
    "    2. **Central Limit Theorem** (For Decision Trees) - **averaging large number of decision trees** that each have high variance to reduce overall variance\n",
    "* Making Predictions - create $B$ bootstrapped training datasets\n",
    "    * **For Regression**: Average predictions of $B$ decision trees\n",
    "        * Equation: $\\hat{f}_{bag}(x)=\\frac{1}{B}\\sum_{b=1}^B \\hat{f}^{*b}(x)$\n",
    "    * **For Classification**: Majority vote among $B$ decision trees\n",
    "* **Error Estimation From Remaining Un-Bootstrap Samples** using OOB:\n",
    "    * Since bootstrapped, each decision tree only uses about $\\frac{2}{3}$ of observations\n",
    "    * Remaining $\\frac{1}{3}$ can be used to estimate **Out of Bag (OOB) Error** (which, can be treated as **test error**)\n",
    "* **Bagging Steps:**\n",
    "    1. Draw a **random bootstrap** sample of size $n$ (randomly choose $n$ samples from the training set with replacement)\n",
    "    2. Grow a decision tree from the bootstrap sample. At each node:\n",
    "        * Split the node using the feature that provides the best split according to the objective function, for instance, by maximizing the information gain\n",
    "    3. Repeat the steps: 1 & 2 *k* times\n",
    "    4. Aggregate the prediction by each tree to assign the class label by **majority vote** (or **average** them for regression)\n",
    "* Problem arising with **only-Bagging** method:\n",
    "    * (-) All bagged decision trees can be the **same**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Random Forest Method (using Bagging)\n",
    "* Uses bagging method to generate multiple decision tree models and averages out the variance through CLT\n",
    "* **Random selection of $m$ predictors/features** - at **each** split, choose a random selection of $m$ predictors/features (subset of predictors/features)\n",
    "    * **Classification tuning:** $(m = \\sqrt{p})$\n",
    "        * Example: For $p=100$ predictors in dataset, randomly get $10$ features at **each** split point\n",
    "    * **Regression tuning:** $(m = \\frac{p}{3})$\n",
    "    * What does choosing a random subset of predictors at each split do? It **decorrelates** the decision trees, thereby leading to improvements in performance over plain vanilla bagging\n",
    "* How to Handle Categorical Data:\n",
    "    * String values need to be converted into numeric\n",
    "    * If possible, convert to a continuous variable (e.g. S, M, L to size in actual weight or height)\n",
    "    * Sklearn doesn't support splitting on multiple features (it will use $\\leq$ for all variables)\n",
    "    * Don't drop one of the categories\n",
    "* Handling Missing Values:\n",
    "    * Typical implementation will use the median value\n",
    "    * Can first use the median values and then use **proximities** to calculate a more accurate missing value later\n",
    "        * **Proximities** - can be used to see how similar 2 data points are\n",
    "        * After fitting the random forest, for each tree in random forest count the number of times that 2 data points are in the same leaf node\n",
    "        * Normalize at the end\n",
    "* Random Forest Steps:\n",
    "    1. Draw a **random bootstrap** sample of size $n$ (randomly choose $n$ samples from the training set with replacement)\n",
    "    2. Grow a decision tree from the bootstrap sample. At each node:\n",
    "        1. **Randomly select $d$ features without replacement**\n",
    "        2. Split the node using the feature that provides the best split according to the objective function, for instance, by maximizing the information gain\n",
    "    3. Repeat the steps: 1 & 2 *k* times\n",
    "    4. Aggregate the prediction by each tree to assign the class label by **majority vote** (or **average** them for regression)\n",
    "* How many decision trees to use? (n_estimators in sklearn)\n",
    "    * Variance decreases with more trees (with diminishing returns)\n",
    "    * Run time scales linearly with more trees\n",
    "    * More is still better, but wait until the end to run 50,000 trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **Feature Importance** - evaluate the importance of features\n",
    "![feature_importance](feature_importance.png)\n",
    "* Calculating Feature Importance in Regression vs. Classification RF Models:\n",
    "    * In **Bagging/Random Forest Regression trees** - record total amount **RSS decreases** due to splits over given predictor, then average over all $B$ decision trees $\\rightarrow$ larger value indicates \"importance\"\n",
    "    * In **Bagging/Random Forest Classification trees** - record total amount **Gini Index decreases** due to splits over given predictor, then average over all $B$ decision trees $\\rightarrow$ larger value indicates \"importance\"\n",
    "* Alternative Way to Calculate Feature Importance\n",
    "    * To evaluate importance of $j$th feature:\n",
    "        1. When $b$th tree is bootstrapped, **OOB** samples passed down through decision tree $\\rightarrow$ record accuracy\n",
    "        2. Values of $j$th feature randomly permuted in **OOB** samples $\\rightarrow$ compute new lower accuracy\n",
    "    * Average **decrease** in accuracy over all decision trees\n",
    "* Comparison of Feature Importances By Split Measurement Metric\n",
    "![gini_rand_feat_impt](gini_rand_feat_impt.png)\n",
    "* Feature Importance in Sklearn\n",
    "    * Basically, the **higher in the decision tree the feature is**, the more important it is in determining the result of a data point\n",
    "    * The **expected fraction of data points that reach a node** is used as an estimate of that feature's importance for that tree\n",
    "    * Finally, average those values across all decision trees to get the feature's importance\n",
    "* Feature Importance Overall:\n",
    "    * Feature importances are almost always put forth as normalized values. It is important that we can compare features to other features.\n",
    "    * Authors of RF state that you should be interested in **rank only and <u>not</u> magnitude**\n",
    "    * Typically, the more features you have in random forest, the less important any individual feature will be\n",
    "    * Highly correlated features tend to split importance\n",
    "    * **Some highly correlated but not super important features will look important**\n",
    "* Bias-Variance Trade off For Random Forest\n",
    "    * **Bias** - by creating full decision trees, we get relatively **low** bias\n",
    "        * Expectation of average of $B$ decision trees same as expectation of any one of the decision trees\n",
    "    * **Variance** - average of $B$ identically distributed random variables with pairwise correlation, $\\rho$, has variance\n",
    "        ![bagging_data](bagging_data.png)\n",
    "        ![bagged_avg](bagged_avg.png)\n",
    "        * Equation: $\\rho\\sigma^2+\\frac{1-\\rho}{B}\\sigma^2$ - **decorrelation of the decision trees** by randomly selecting of $m$ features at each split/node\n",
    "        * Greater the $B$ (number of decision trees), the more the variance will reduce (although after some point there is diminishing return / computationally expensive)\n",
    "* Recommended Tuning For Random Forest:\n",
    "    * **Classification:** \n",
    "        * minimum node size = 1\n",
    "        * max_features: $m=\\sqrt{p}$\n",
    "    * **Regression:** \n",
    "        * minimum node size = 5\n",
    "        * max_features: $m=\\frac{p}{3}$ or full features\n",
    "    * **min_sample_leaf** - start with None and try others\n",
    "    * **n_jobs** - choosing $-1$ will make it run on maximum # of processors\n",
    "    * **k-Fold Cross Validation** to get optimal hyperparameters\n",
    "* Random Forest Pros:\n",
    "    * (+) For an out of the box model, it has very good accuracy\n",
    "    * (+) Trees can be trained in parallel to make computations faster\n",
    "    * (+) OOB estimates allow for an estimate of generalization error without needing CV\n",
    "    * (+) Can handle thousands of features and be used for feature reduction\n",
    "* Random Forest Intuition:\n",
    "    * Cannot extrapolate well for regression trees\n",
    "    * Just because we have interactions, doesn't mean you'll never want interaction variables\n",
    "    * Just because we have OOB error, doesn't mean you shouldn't do CV\n",
    "    * Start with a small number of trees at first, then increase\n",
    "    * Pickling makes a giant file (~GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
